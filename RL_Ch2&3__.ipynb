{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL_Ch2&3__",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/syeong1218/RL/blob/master/RL_Ch2%263__.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfVnwiMZ--LU",
        "colab_type": "text"
      },
      "source": [
        "## **+ 피드백**\n",
        "2020년 5월 16일 오전 10시 30분 교수와의 면담을 통해 논문 진행방향을 원래 하던 방식에서 다른 방식으로 바꾸었습니다. 참고문헌의 책을 요약하는 것이 아니라 저희가 이해한 내용을 중심으로 작성하기로 했습니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISGqpSpuCMU2",
        "colab_type": "text"
      },
      "source": [
        "# 2장 강화학습 개념"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gcpHd086OJi",
        "colab_type": "text"
      },
      "source": [
        "# 2.1 강화학습 개요\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zbvy2jPE6OHn",
        "colab_type": "text"
      },
      "source": [
        "**강화학습**은 일반적으로 불연속적인 **이산시간을 시간변수**로 하여, 시간 간격마다 **행동**이 가해지는 순차적인 **의사 결정 문제**를 구성한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTb3bbtR6OB9",
        "colab_type": "text"
      },
      "source": [
        "### 강화학습 구성요소"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gM_Tncx6lH3",
        "colab_type": "text"
      },
      "source": [
        "* 에이전트 : 의사 결정 주체\n",
        "* 환경 : 에이전트를 제외한 요소(시스템)\n",
        "* 행동\n",
        "* 상태\n",
        "* 보상"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ai6Gn3X76sd8",
        "colab_type": "text"
      },
      "source": [
        "### 강화학습 과정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Op7Owxd66uXw",
        "colab_type": "text"
      },
      "source": [
        "먼저, 에이전트가 **상태를 관측**하여 **정책**(특정 행동을 택하게 하는 확률적인 규칙)에 따라 여러 행동 중 특정한 **행동**을 하여 **환경**을 바꾸어 **새로운 상태**에 도달하며, 이에 따라 **보상**을 받는다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHm3xXga7He5",
        "colab_type": "text"
      },
      "source": [
        "### 강화학습 목표"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V28Mh4Fh7KwV",
        "colab_type": "text"
      },
      "source": [
        "강화학습이 종료되는 시점까지 **누적된 총 보상을 최대화**하는 것"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYpwsutt7cvo",
        "colab_type": "text"
      },
      "source": [
        "### 강화학습 특징"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFnef_nP7a83",
        "colab_type": "text"
      },
      "source": [
        "시간 간격마다 최대의 보상을 얻는 행동을 선택하는 것이 아니라, 총 누적 보상이 최대가 되는 행동을 선택하는 것이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTKqqsDF7r0l",
        "colab_type": "text"
      },
      "source": [
        "## 강화학습 vs 최적 제어"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GE2xFd-17t1Z",
        "colab_type": "text"
      },
      "source": [
        "**최적 제어** : 제어 시스템으로부터 **출력 값을 측정**하여 설계자가 **원하는 대로** 시스템이 **동작할 수 있도록 제어 입력을 결정**하는 것"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QBEmpEV7_VA",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### 강화학습과 최적 제어의 용어 비교"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfLHOSy77_Op",
        "colab_type": "text"
      },
      "source": [
        "* **에이전트**(강화학습)=**제어기**(최적 제어)\n",
        "* **행동**(강화학습)=**제어 입력**(최적 제어)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emJCo2xt7_Bg",
        "colab_type": "text"
      },
      "source": [
        "### 차이점"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLDttIkI7-aZ",
        "colab_type": "text"
      },
      "source": [
        "**최적 제어**는 **수학적 동역학 모델**에 따라 어떻게 동작을 수행할지 예측한다.\n",
        "<br>**강화학습**에서 환경(시스템)은 수학적 모델을 따르지 않고, 오직 **시스템에서 얻은 데이터**에 의존한다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsvD2wn8CRj3",
        "colab_type": "text"
      },
      "source": [
        "   - 예시 : 최소의 에너지로 장애물 사이를 피해 통과하는 드론\n",
        "\n",
        "<br>목표 -  드론이 장애물을 피해가면서 최소의 에너지를 사용해 목표 지점에 도달하도록 드론에 가해지는 제어 입력 또는 행동을 순차적으로 결정하는 것\n",
        "\n",
        "![대체 텍스트](https://github.com/syeong1218/RL/blob/master/2-3.jpg?raw=true)\n",
        "\n",
        "| |최적제어|강화학습|\n",
        "|----|----|----|\n",
        "|비행 제어 컴퓨터|제어기|에이전트|\n",
        "|컴퓨터를 제외한 모든 것|시스템|환경|\n",
        "||수학적 모델에 의존|보상에 의존|\n",
        "\n",
        "![대체 텍스트](https://github.com/syeong1218/RL/blob/master/2-4.jpg?raw=true)\n",
        "=> **최적 제어** : 드론의 위치, 방향, 속도, 장애물의 위치 등 여러 **상태변수의 변화**를 시간의 함수로 표현하여 **수학적 모델**을 만들어 장애물까지의 거리에 따른 **비용함수를 설정**하여 이를 **최소로 하는 함수**에 따라 장애물 사이를 통과한다.\n",
        "\n",
        "=> **강화학습** : 각종 **센서**를 이용하여 여러 **상태변수를 측정**하여, 드론의 힘과 크기의 방향을 바꿔 드론을 움직여서 장애물까지의 거리가 멀수록 **보상**을 크게 설정하여 진행한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kl7fU9hR6hth",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# 2.2 강화학습 프로세스 및 표기법"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42HECaiqUD4u",
        "colab_type": "text"
      },
      "source": [
        "## 강화학습 프로세스"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kFa6UBGCUg7",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "![대체 텍스트](https://github.com/syeong1218/RL/blob/master/2-5.jpg?raw=true)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oJxwyR29eiq",
        "colab_type": "text"
      },
      "source": [
        "1. 에이전트는 **환경의 상태**($X_t$)를 **측정**\n",
        "2. **정책**으로 인해 **선택된 행동**은 **환경에 인가**\n",
        "3. 행동에 의해 환경의 상태는 **다음 상태**($X_{t+1}$)로 **전환**\n",
        "4. 전환된 환경의 상태로부터 **새로운 행동 선택**\n",
        "5. 전환된 환경으로부터 얻는 **보상을 통해 정책을 계속 개선**  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTZCSQhS9nqs",
        "colab_type": "text"
      },
      "source": [
        "## 강화학습 표기법"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVxWFI1r9qxl",
        "colab_type": "text"
      },
      "source": [
        "* 이산공간의 상태 :  $S_t$\n",
        "* 이산공간의 행동 : $a_t$\n",
        "\n",
        "* 연속공간의 상태 : $X_t$\n",
        "* 연속공간의 행동 : $u_t$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiqYq0_HFCfA",
        "colab_type": "text"
      },
      "source": [
        "# 2.3 마르코프 결정 프로세스(MDP)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NbGXMQIFHOJ",
        "colab_type": "text"
      },
      "source": [
        "## 2.3.1 정의"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KT4zoBTFFHKr",
        "colab_type": "text"
      },
      "source": [
        "### **마르코프 결정 프로세스(MDP)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw_uvuefpGGY",
        "colab_type": "text"
      },
      "source": [
        "\n",
        " : 상태($X_t$), 상태천이 확률밀도함수($p$), 행동($U_t$), 보상함수($r(X_t,U_t)$)로 이루어진 이산시간 확률 프로세스"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BN97EehRumlq",
        "colab_type": "text"
      },
      "source": [
        "## 2.3.1.1 확률적 MDP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1wJHC09urFU",
        "colab_type": "text"
      },
      "source": [
        "$\\rightarrow$  환경 모델이 상태천이 확률밀도함수로 주어지는 경우 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhLYQ85XFHHQ",
        "colab_type": "text"
      },
      "source": [
        "### **상대천이 확률밀도함수**($p$) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2VaKdyfpKIR",
        "colab_type": "text"
      },
      "source": [
        ": 어떤 상태($X_t$)에서 에이전트가 행동($U_t$)을 했을 떄, 다음 상태($X_{t+1}$)로 갈 확률밀도함수"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba6o_QiYFHDk",
        "colab_type": "text"
      },
      "source": [
        "연속공간 상태 및 행동변수 -> $p(X_{t+1} \\mid X_t, U_t) $(확률밀도함수)\n",
        "<br> 이산공간 상태 및 행동변수 -> $P(X_{t+1} \\mid X_t, U_t) $(확률)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvDpowdkFG_o",
        "colab_type": "text"
      },
      "source": [
        "이 함수는 미래의 상태(다음의 상태)가 과거의 상태와 행동에 관계없이 오직 현재 상태와 행동에만 영향을 받기 때문에, 마르코프 시퀀스이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-o-BAQdo5-G",
        "colab_type": "text"
      },
      "source": [
        "### **조건부 확률밀도함수($\\pi$)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T76zrtpSow80",
        "colab_type": "text"
      },
      "source": [
        ": MDP에서 **누적된 보상**을 **가장 많이 획득**하기 위해 각 상태에서 **어떤 행동을 취할 것인지**를 나타내는 함수"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-jBTRybow5V",
        "colab_type": "text"
      },
      "source": [
        "$$\\pi(U_t \\mid X_t) =p(U_t \\mid X_t)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbjWnr2oow3A",
        "colab_type": "text"
      },
      "source": [
        "$\\pi(U_t \\mid X_t)$ 를 **정책** 이라고 하며, **한 상태변수**에서 **여러개의 행동**을 **선택** 할 수 있으며, 각 행동의 확률밀도함수에 의해 선택된다.\n",
        "<BR>이를 다른말로 **확률적 정책**이라고도 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqyOxDYdow0Y",
        "colab_type": "text"
      },
      "source": [
        "![image](https://user-images.githubusercontent.com/53015968/81427077-2aa78e00-9195-11ea-8317-5a97c44db091.png)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pC3J6W47owxd",
        "colab_type": "text"
      },
      "source": [
        "위 그림은 상태변수 $X_0$에서 어떤 정책 $\\pi$에 의해 행동 $U_0$가 확률적으로  선택되면, 상태천이 확률밀도함수 $p(X_{1}\\mid X_0,U_0)$에 의해 $X_1$으로 이동하고 이때, 보상 $r(X_0,U_0)$이 주어지며 이 과정이 반복되는 MDP 전개 그림이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXFhlYs4owud",
        "colab_type": "text"
      },
      "source": [
        "**궤적($\\tau$)** : **상태변수**와 **행동**의 **연속**적인 **시퀀스**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67E2pk2Itsmc",
        "colab_type": "text"
      },
      "source": [
        "$$\\tau = (X_0,U_0,X_1,U_1,\\ldots,X_\\tau,U_\\tau)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuYiQ5n3u3Bq",
        "colab_type": "text"
      },
      "source": [
        "## 2.3.1.2 확정적 MDP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iN8SPrHt_hS",
        "colab_type": "text"
      },
      "source": [
        "$\\rightarrow$ 환경모델과 정책이 모두 확정적으로 주어지는 경우"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwnNlL7-vL7l",
        "colab_type": "text"
      },
      "source": [
        "**환경 모델** :  $X_{t+1}=f(X_t,U_t)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUCQlFxuuT18",
        "colab_type": "text"
      },
      "source": [
        "이는, 시간스텝 $t$에서 상태와 행동이 주어지면 다음상태를 확정적으로 알 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDrkgRQfuT4-",
        "colab_type": "text"
      },
      "source": [
        "**정책** : $U_t=\\pi(X_t)$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrnsGhWLuTy5",
        "colab_type": "text"
      },
      "source": [
        "정책은 상태변수에서 행동을 **직접 계산**한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3jxt9TfuTv_",
        "colab_type": "text"
      },
      "source": [
        "### **반환값($G_t$)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cs2OnG_2uTtF",
        "colab_type": "text"
      },
      "source": [
        "$\\rightarrow$ 시간스텝 $t$일 때, 얻을 수 있는 **보상의 총합**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76Ym3_U1uTpI",
        "colab_type": "text"
      },
      "source": [
        "$$G_t=\\sum_{k=t}^T  \\gamma^{k-t}r(X_k,U_k)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFsU-AW8uTmM",
        "colab_type": "text"
      },
      "source": [
        "$\\gamma \\in [0,1]$은 **감가율**이다. 값이 작을수록 보상을 가까운 미래에 받는다. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXd7d-Pn_ab7",
        "colab_type": "text"
      },
      "source": [
        "$\\gamma$=0이면, $G_t=r(X_t,U_t)$이므로, 오직 가까운 미래에 대해 받는 보상이다.\n",
        "<br> $\\gamma$=1이면, $G_t=\\sum_{k=t}^T r(X_k,U_k)$이므로, 먼 미래에 대해 받는 보상과 연관이 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrEi5fdYuTjw",
        "colab_type": "text"
      },
      "source": [
        "### **에피소드**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t87CTO45uThU",
        "colab_type": "text"
      },
      "source": [
        ": 어떤 정책을 시행해 $X_0 \\rightarrow U_0 \\rightarrow r(X_0,U_0)\\rightarrow \\ldots\\rightarrow X_T\\rightarrow U_T$의 순서로 전개될 때, 이러한 **상태변수, 행동, 보상의 시퀀스 집합**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ua6EdqpnuTd5",
        "colab_type": "text"
      },
      "source": [
        "유한 구간 에피소드 : 특정 상태변수에 도달하는 등 목적이 성취되면 종료되는 에피소드($t=T$)\n",
        "<BR>무한 구간 에피소드 : $T=\\infty$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ek8Ok8WuTYz",
        "colab_type": "text"
      },
      "source": [
        "## 2.3.2 가치함수 - 상태가치 & 행동가치"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkaLffaouTVw",
        "colab_type": "text"
      },
      "source": [
        "### 2.3.2.1 상태가치"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdHHpkS91BPZ",
        "colab_type": "text"
      },
      "source": [
        "![image](https://user-images.githubusercontent.com/53015968/81427186-66425800-9195-11ea-9d18-f9c1169d1fae.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TT2T9BJF0opw",
        "colab_type": "text"
      },
      "source": [
        "$\\rightarrow$ 상태변수($X_t$)에서 정책 $\\pi$에 의해 **행동이 가해졌을 때**, 기대할 수 있는 **반환값**.  즉, 미래 보상의 총합"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDPyV2AN1ZUx",
        "colab_type": "text"
      },
      "source": [
        "상태가치함수 식 : $V^\\pi(X_t)=E_{\\tau_{u_t:u_T \\sim p(\\tau_{u_t:u_T\\mid X_t})}}[\\sum_{k=t}^T \\gamma^{k-t}r(X_k,u_k)\\mid X_t]$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5A09CoowuTTI",
        "colab_type": "text"
      },
      "source": [
        "### 2.3.2.2 행동가치"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNYRMFgb3Sde",
        "colab_type": "text"
      },
      "source": [
        "![image](https://user-images.githubusercontent.com/53015968/81427917-6d1d9a80-9196-11ea-97ea-f00797babb79.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucMYcvvd3T_R",
        "colab_type": "text"
      },
      "source": [
        "$\\rightarrow$ 상태변수($X_t$)에서  **행동($u_t$)을 선택하고**, 그로부터 정책($\\pi$)에 의해 행동이 가해졌을 때,기대할 수 있는 **반환값**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_YnhiHN3xwF",
        "colab_type": "text"
      },
      "source": [
        "행동가치 식 :  $Q^\\pi(X_t,u_t)=E_{\\tau_{x_{t+1}:u_T \\sim p(\\tau_{u_{t+1}:u_T\\mid X_t,u_t})}}[\\sum_{k=t}^T \\gamma^{k-t}r(X_k,u_k)\\mid X_t,u_t]$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtOEgeko30fI",
        "colab_type": "text"
      },
      "source": [
        "이 두 식을 이용하면 다음과 같은 결론을 얻을 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NnxxRh230cw",
        "colab_type": "text"
      },
      "source": [
        "$V^\\pi(X_t)=E_{u_t \\sim \\pi(u_t \\mid X_t)}[Q^\\pi (X_t,u_t)]$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ap4ayAWY30aP",
        "colab_type": "text"
      },
      "source": [
        "즉, **상태가치**는 상태변수$X_t$에서 **선택 가능한 모든 행동**$u_t$에 대한 **행동가치의 기댓값(평균값)**이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHqXnNtM30GD",
        "colab_type": "text"
      },
      "source": [
        "## 2.3.3 벨만 방정식"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFyaLHQL441I",
        "colab_type": "text"
      },
      "source": [
        ": **현재** 상태변수의 **가치**와 **다음** 시간스텝의 상태변수의 **가치**와의 관게"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukWuzlfkDjRW",
        "colab_type": "text"
      },
      "source": [
        "벨만 방정식은 상태가치 함수와 행동가치 함수로 각각 이루어진 식으로 구분할 수 있다. \n",
        "<br> 두 식 모두 각각의 가치함수에 대해 현재 상태변수 가치와 다음 시간스텝의 상태변수 가치의 관계를 나타낸다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zv4quox3D6SJ",
        "colab_type": "text"
      },
      "source": [
        "예시로 상태가치함수로 이루어진 벨만 방정식은 아래와 같다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeZhytoa45Qb",
        "colab_type": "text"
      },
      "source": [
        "$V^\\pi(X_t)=E_{u_t \\sim \\pi(u_t \\mid X_t)}[r(X_t,u_t)+E_{x_{t+1} \\sim p(X_{t+1} \\mid X_t,u_t)}[rV^\\pi(X_{t+1})]]$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCLX91nE45N4",
        "colab_type": "text"
      },
      "source": [
        "여기서 상태가치 함수($V^\\pi(X_t), V^\\pi (X_{t+1})$)는 원래 시변함수이므로 두 상태가치 함수는 다르다.\n",
        "<br>명확히 하기위해 위와 같이 명백히 표시해야한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alQOGakuAbhP",
        "colab_type": "text"
      },
      "source": [
        "하지만, 편의상  $T \\rightarrow \\infty$ (무한구간)이라면 두 상태 함수는 시불변함수가 되어 동일하게 된다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAIIErh045LY",
        "colab_type": "text"
      },
      "source": [
        "## 2.3.4 벨만 최적 방정식"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsqt2qUDFJ90",
        "colab_type": "text"
      },
      "source": [
        ": 벨만 방정식과 비슷한 원리로, **현재의 최적 가치**와 **다음 시간스텝의 최적 가치**의 관계를 나타낸 식"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQPSsUxJ45I_",
        "colab_type": "text"
      },
      "source": [
        "**최적 상태가치 함수** : 모든 정책 중 상태가치 값을 최대로 만드는 정책(최적 정책)을 적용한 상태가치 함수\n",
        "<br>**최적 행동가치 함수** : 모든 정책 중 상태가치 값을 최대로 만드는 정책(최적 정책)을 적용한 행동가치 함수"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxAXPC3BuTQQ",
        "colab_type": "text"
      },
      "source": [
        "최적 상태가치 함수는 최적 행동가치 함수와 아래와 같은 관계를 갖는다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pcEFNFQCC-B",
        "colab_type": "text"
      },
      "source": [
        "$V^*(X_t)=maxQ^*(X_t,u_t)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMZV7LygErGo",
        "colab_type": "text"
      },
      "source": [
        "즉, 최적 행동가치 함수의 최댓값이 최적 상태가치 함수이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FC_z6YyVCWhI",
        "colab_type": "text"
      },
      "source": [
        "이렇게 상태가치의 값을 최대로 만드는 정책을 **최적 정책**($\\pi^*(u_t \\mid X_t)$) 이라고 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMsk5m_oCtNH",
        "colab_type": "text"
      },
      "source": [
        "##2.4 강화학습 방법"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbljO-fxHLsi",
        "colab_type": "text"
      },
      "source": [
        "### 강화학습 문제 해결 방법들의 공통점"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAL2uidkC_vN",
        "colab_type": "text"
      },
      "source": [
        "![2 9](https://user-images.githubusercontent.com/53015968/81430084-d05cfc00-9199-11ea-9deb-00c641339c66.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyDUe99VDA5r",
        "colab_type": "text"
      },
      "source": [
        "1. 정책 실행을 통한 데이터 생성\n",
        "2. 모델 또는 가치함수의 추정\n",
        "3. 정책 개선 \n",
        "<br>1 ~ 3단계 반복 $\\rightarrow$ 최적의 정책 산출"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3q8kkNeHXVz",
        "colab_type": "text"
      },
      "source": [
        "### 강화학습 문제 해결 방법"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXmQOIKEHbIo",
        "colab_type": "text"
      },
      "source": [
        "1. 가치함수 추정\n",
        "2. 정책 직접 유도\n",
        "3. 환경 모델 추정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVeFmkX5DHjx",
        "colab_type": "text"
      },
      "source": [
        "### 2.4.1 가치함수 추정 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SD_xCXcIEE6w",
        "colab_type": "text"
      },
      "source": [
        "$\\rightarrow$ **가치함수를 추정**하여 **최대의 보상**을 계산하여 **최적의 정책**을 얻는다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0B513XOEhKJ",
        "colab_type": "text"
      },
      "source": [
        "#### 가치 기반 강화학습 방법"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8H40ygIFBAC",
        "colab_type": "text"
      },
      "source": [
        "![2 10](https://user-images.githubusercontent.com/53015968/81430090-d226bf80-9199-11ea-8652-e6b5b5dce33c.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnulGV_IEMzJ",
        "colab_type": "text"
      },
      "source": [
        "1. 행동가치 함수를 추정\n",
        "2. 각 상태에서 행동가치 함수를 최대화하는 행동 선택\n",
        "3. 최적 정책 유도"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Z9_VEAzEx-D",
        "colab_type": "text"
      },
      "source": [
        "### 2.4.2 직접 정책 유도"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79qbxMTdFUX_",
        "colab_type": "text"
      },
      "source": [
        "목적 :  **보상의 기댓값을 최대**로 하기 위해 **정책($\\pi_\\theta (u_t \\mid X_t)$)**을 최적화하는 **정책 파라미터** $\\theta$를 계산"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Rp6f50BFxit",
        "colab_type": "text"
      },
      "source": [
        "#### 정책 그래디언트 방법"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnFKJC93E3OG",
        "colab_type": "text"
      },
      "source": [
        "![2 11](https://user-images.githubusercontent.com/53015968/81430093-d357ec80-9199-11ea-9e30-d9dc1fda779a.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzJtLcyBF1FM",
        "colab_type": "text"
      },
      "source": [
        "위 방법은 보통 가치함수도 함께 추정하여 정책의 성과를 평가하는 **Actor-Critic** 구조를 사용한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15R1F6RDG4jN",
        "colab_type": "text"
      },
      "source": [
        "+  Actor-Critic : 강화학습의 MDP 중 정책과 가치함수를 Deep Neural Network로 parameterization(파라미터를 이용해 전체를 예측)하여 근사화한 기법"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8sSGo5-FEan",
        "colab_type": "text"
      },
      "source": [
        "### 2.4.3 환경 모델 추정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4Y9kxdRHyys",
        "colab_type": "text"
      },
      "source": [
        "#### 모델 기반 강화학습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsG2TVEvIFlL",
        "colab_type": "text"
      },
      "source": [
        "![2 12](https://user-images.githubusercontent.com/53015968/81430098-d4891980-9199-11ea-83e2-6cd439c72da4.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxXCROjiH1ec",
        "colab_type": "text"
      },
      "source": [
        "모델 기반 강화학습은 환경 모델을 비용함수를 이용해 추정하여 최적 비용 경로를 찾아서 정책을 개선하는 간단하고 효율적인 방법이다. 보통 앞의 장애물을 피하는 드론제어 예시와 같이 로봇제어, 드론제어에 많이 사용된다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dY4GKlEHokaD",
        "colab_type": "text"
      },
      "source": [
        "# 3장 정책 그래디언트  \n",
        "\n",
        "## 3.1 배경  \n",
        "\n",
        "정책 그래디언트 : 정책(policy)를 파라미터화하고, 누적 보상과 정책 파라미터 간의 관계를 구축하고 최적화 방법을 통해 누적 보상을 최대화하는 정책 파라미터를 계산하는 강화학습의 방법"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zC-_xIFhoqrI",
        "colab_type": "text"
      },
      "source": [
        "## 3.2 목적함수  \n",
        "\n",
        "목적함수 **J** : 반환값의 기댓값으로 이루어진 함수\n",
        "\n",
        "=> 목표 : $\\theta$ 로 파라미터화된 $\\pi_\\theta(u_t|x_t)$ 을 사용하여 목적함수를 최대로 만드는 $\\theta$ 를 계산하는 것.\n",
        "\n",
        "$$\\theta^*=argmaxJ(\\theta)$$\n",
        "$$J(\\theta)=E_{\\tau \\sim p_\\theta(\\tau)}[\\sum_{t=0}^T \\gamma^t r(x_t,u_t)] $$  \n",
        "$r(x_t, u_t)$ : 시간스텝 $t$일 때 상태변수가 $x_t$에서 행동 $u_t$를 사용했을 때 에이전트가 받는 보상\n",
        "\n",
        "$\\gamma\\in[0,1]$ : 감가율(discount factor)\n",
        "\n",
        "$p_\\theta(\\tau)$ : 기댓값을 계산할 때 확률밀도함수로 $p_\\theta(\\tau)$를 사용한다는 의미  \n",
        "\n",
        "$\\tau$ : 정책 $\\pi_\\theta$로 생성되는 궤적 $\\tau=(x_0, u_0, x_1, u_1, x_2, u_2, x_3, u_3, \\cdots, x_T, u_T)$\n",
        "\n",
        "$p_\\theta(\\tau)$ : 정책 $\\pi_\\theta$로 생성되는 궤적의 확률밀도함수\n",
        "\n",
        "![KakaoTalk_20200509_145820465](https://user-images.githubusercontent.com/53211502/81465554-c4a71f00-9205-11ea-852f-4aa6c861bb7b.jpg)  \n",
        "\n",
        "- 정책 신경망\n",
        "\n",
        "파라미터 $\\theta$는 그림 3.2에 도시한 것 같이 신경망의 모든 가중치다.\n",
        "![KakaoTalk_20200509_151311299](https://user-images.githubusercontent.com/53211502/81465828-f0c39f80-9207-11ea-968a-db245cc2df55.jpg)\n",
        "\n",
        "- MDP에서의 에이전트의 정책 신경망과 환경의 상호작용\n",
        "![KakaoTalk_20200509_152306979](https://user-images.githubusercontent.com/53211502/81465989-0e453900-9209-11ea-96c8-92d6c716625e.jpg)\n",
        "\n",
        "$G_0 = \\sum_{t=0} ^T \\gamma^t r (x_t, u_t)$ : 시간 스텝 $t=0$부터 에피소드가 종료될 때까지 받을 수 있는 전체 궤적에 대한 감가 보상(discount reward)의 총합 = $G_0$\n",
        "\n",
        " $G_k = \\sum_{k=t}^T \\gamma^{k-t} r (x_k, u_k)$ : 임의의 시간 $k=t$부터 에피소드가 종료될 때까지 받을 수 있는 예정 보상(reward-to-go)의 총합\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wO36mh2npLMp",
        "colab_type": "text"
      },
      "source": [
        "$p_\\theta (\\tau) = p_\\theta(x_0, u_0, x_1, u_1, \\cdots. x_T, u_T)$  \n",
        "전개된 $p_\\theta (\\tau)$를 베이즈 정리와 마르코프 시퀀스 가정에 의해 다음과 같이 표현할 수 있다.  \n",
        "$$p_\\theta (\\tau) = p(x_0) \\prod_{t=0}^T \\pi_\\theta(u_t|x_t)p(x_{t+1}|x_t, u_t)$$  \n",
        "\n",
        "*  베이즈 정리  \n",
        "$$P\\{A\\} = \\sum_{i=1}^n P\\{A, B_i\\} = \\sum_{i=1}^n P\\{A|B_i\\} P\\{B_i\\}$$  \n",
        "* 마르코프 시퀀스 가정  \n",
        " 현재의 state가 history의 모든 관련 정보를 갖고 있다.  \n",
        "\n",
        "목적함수 $J$를 전개하면 다음과 같다.  \n",
        "$$J(\\theta)=E_{\\tau \\sim p_\\theta(\\tau)}[\\sum_{t=0}^T \\gamma^t r(x_t,u_t)] \n",
        "           =\\int_\\tau p_\\theta(\\tau)(\\sum_{t=0} ^T \\gamma^t r (x_t, u_t))d\\tau $$  \n",
        "여기서 궤적을 $x_0$과 나머지, 두 영역으로 분할하고 베이즈 정리를 적용하면 다음과 같다.  \n",
        "$$p_\\theta (\\tau) = p(x_0) p_\\theta(\\tau_{u_0 : u_T}|x_0)$$\n",
        "위 식을 목적함수 전개 식에 대입 후 상태가치 함수 $V^{\\pi_\\theta}$로 묶어 주면 목적함수는 다음과 같다.  \n",
        "$$J(\\theta) = E_{x_0 \\sim p(x_0)}[V^{\\pi_\\theta}(x_0)]$$  \n",
        "즉 **목적함수**는 초기 상태변수 **$x_0$에 대한 상태가치의 평균값**이 된다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFraaedHot6z",
        "colab_type": "text"
      },
      "source": [
        "## 3.3 정책 그래디언트  \n",
        "- 목적함수를 최대로 만드는 $\\theta$를 계산\n",
        "  1. 목적함수를 $\\theta$로 미분  \n",
        "$$ {{\\partial J(\\theta)}\\over{\\partial \\theta}}= \\nabla_\\theta J(\\theta) = \\int_\\tau \\nabla_\\theta p_\\theta(\\tau) \\sum_{t=0}^T \\gamma^t r (x_t, u_t) d\\tau=\\int_\\tau p_\\theta(\\tau) \\nabla_\\theta \\log p_\\theta(\\tau) \\sum_{t=0}^T \\gamma^t r (x_t, u_t) d\\tau$$  \n",
        "\n",
        "  2. $\\nabla_\\theta \\log p_\\theta(\\tau) = {{\\nabla_\\theta p_\\theta(\\tau)}\\over{p_\\theta(\\tau)}}$를 이용 \n",
        "$$\\nabla_\\theta \\log p_\\theta(\\tau) = \\nabla_\\theta (\\log p(x_0) + \\sum_{t=0}^T \\log \\pi_\\theta (u_t|x_t) + \\log p(x_{t+1}|x_t, u_t))$$  \n",
        "\n",
        "  3. 좌변의 두번째 항만 $\\theta$의 함수로 단순화\n",
        "$$\\nabla_\\theta \\log p_\\theta(\\tau) =  \\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta (u_t|x_t)$$ \n",
        "\n",
        "  4. 목적함수의 그래디언트 식에 대입  \n",
        "$$\\nabla_\\theta J(\\theta)=E_{\\tau \\sim p_\\theta(\\tau)}[\\sum_{t=0}^T (\\nabla_\\theta \\log \\pi_\\theta (u_t|x_t)(\\sum_{k=0}^T\\gamma^k r (x_k, u_k)))] $$  \n",
        "=> 환경 모델이 필요 없는 모델 프리(model-free) 강화학습 방법\n",
        "\n",
        "- 시간스텝 t<k에서와 무한 구간 에피소드 고려\n",
        "\n",
        "  => 예정 보상에만 감가율을 적용\n",
        "$$\\nabla_\\theta J(\\theta)=E_{\\tau \\sim p_\\theta(\\tau)}[\\sum_{t=0}^T (\\nabla_\\theta \\log \\pi_\\theta (u_t|x_t)(\\sum_{k=t}^T\\gamma^{k-t} r (x_k, u_k)))] $$  \n",
        " \n",
        "- 경사상승법을 이용하여 $\\theta$ 업데이트\n",
        "$$\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta J(\\theta)$$  \n",
        "\n",
        "정책 그래디언트에 사용되는 목적함수 그래디언트를 다음과 같다.  \n",
        "* 목적함수 : $J(\\theta)=E_{\\tau \\sim p_\\theta(\\tau)}[\\sum_{t=0}^T \\gamma^t r(x_t,u_t)], \\tau=(x_0, u_0. x_1. u_1, \\cdots, x_T, u_T)$  \n",
        "* 가정 : 확률적 정책, $u_t \\sim \\pi_\\theta (u_t|x_t)$  \n",
        "* 그래디언트 : $\\nabla_\\theta J(\\theta)=E_{\\tau \\sim p_\\theta(\\tau)}[\\sum_{t=0}^T (\\nabla_\\theta \\log \\pi_\\theta (u_t|x_t)(\\sum_{k=t}^T\\gamma^{k-t} r (x_k, u_k)))] $\n",
        "* 업데이트 : $\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta J(\\theta)$ \n",
        "\n",
        "=> 정책 그래디언트는 파라마터 **$\\theta$를 업데이트 하는 것**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrwGvtcco67P",
        "colab_type": "text"
      },
      "source": [
        "## 3.4 REINFORCE 알고리즘  \n",
        "\n",
        "반환값 $G_t$이용\n",
        "\n",
        " $$\\nabla_\\theta J(\\theta) \\approx \\nabla_\\theta {1 \\over M} \\sum_{m=1}^M[\\sum_{t=0}^T ( \\log \\pi_\\theta (u_t^{(m)}|x_t^{(m)})G_t^{(m)})]$$  \n",
        "\n",
        " M개의 에피소드를 이용해 목적함수의 그래디언트를 계산하는 식을 도시하면 다음과 같다.  \n",
        " ![KakaoTalk_20200509_171445690](https://user-images.githubusercontent.com/53211502/81468127-a4cd2680-9218-11ea-9432-fa672b5d0661.jpg)\n",
        "\n",
        "한 개의 에피소드를 이용해 목적함수의 그래디언트를 계산하는 방법을 도시하면 다음과 같다.  \n",
        "![KakaoTalk_20200509_171808860](https://user-images.githubusercontent.com/53211502/81468209-202ed800-9219-11ea-8bc1-8fb3fdaa080b.jpg)\n",
        "\n",
        "파라미터 $\\theta$로 표현된 정책 $\\pi_\\theta$을 신경망으로 구성할 때  \n",
        "$\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta J(\\theta) \\approx \\alpha \\nabla_\\theta \\sum_{t=0}^T ( \\log \\pi_\\theta (u_t^{(m)}|x_t^{(m)})G_t^{(m)})$이기 때문에  \n",
        "에피소드의 손실함수로 $loss = -\\sum_{t=0}^T ( \\log \\pi_\\theta (u_t^{(m)}|x_t^{(m)})G_t^{(m)})$를 사용한다.  \n",
        "따라서 반환값을 크게 받은 정책의 에피소드는 목적함수의 그래디언트 계산 시 더 큰 영향을 끼치고 반환값이 작은 정책의 에피소드는 작은 영향을 끼쳐, 점진적으로 정책이 개선된다.  \n",
        "\n",
        "- REINFORCE 알고리즘의 프로세스\n",
        "\n",
        "![KakaoTalk_20200509_173230345](https://user-images.githubusercontent.com/53211502/81468581-6be28100-921b-11ea-97c4-572781ac3eae.jpg)\n",
        "\n",
        "  1. 정책 $\\pi_{\\theta}(u|x)$로 부터 샘플 궤적 $\\tau^{(m)}=\\left \\{ x_{0}^{(i)},u_{0}^{(m)},x_{1}^{(m)},u_{1}^{(m)},\\cdots ,x_{T}^{(m)},u_{T}^{(m)} \\right \\}$ 생성 \n",
        "  2. 에피소드에서 반환값 $G_t^{(m)}$ 계산\n",
        "  3. 에피소드에서 손실함수를 계산\n",
        "\n",
        "  $loss = -\\sum_{t=0}^{T}(log\\pi _{\\theta }(u_{t}^{(m)}|x_{t}^{(i)})G_{t}^{(m)})$\n",
        "  4. 정책 파라미터 업데이트\n",
        "\n",
        "  $\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta J(\\theta)$\n",
        "  5. 위의 과정 반복  \n",
        "\n",
        "- REINFORCE 알고리즘 장점 \n",
        "  $\\theta$를 조금씩 바꿔가며 학습하기 때문에 stable하다\n",
        "- REINFORCE 알고리즘 문제점\n",
        "  1. 한 에피소드가 끝나야 정책을 업데이트할 수 있다.  \n",
        "  $\\rightarrow$ 신경망 학습 시간이 상당히 길어질 수 있다.  \n",
        "  2. 그래디언트의 분산이 매우 크다.  \n",
        "  $\\rightarrow$ 반환값이 에피소드마다 크게 차이가 남에 따라 목적함수의 그래디언트도 이에 비례하여 값이 들쭉날쭉하므로 신경망 학습에 시간이 많이 걸리고 학습이 전혀 안 될 수도 있다.  \n",
        "  3. 온-플리시(on-policy) 방법이다.  \n",
        "  $\\rightarrow$ 정책을 업데이트하기 위해서 해당 정책을 실행시켜 발생한 에피소드가 필요하므로 효율성이 매우 떨어진다.  \n"
      ]
    }
  ]
}