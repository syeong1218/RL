{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL_Ch2&3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP/g+IUr8voAr89AVid7rZv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/syeong1218/RL/blob/master/RL_Ch2%263.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISGqpSpuCMU2",
        "colab_type": "text"
      },
      "source": [
        "# 2장 강화학습 개념"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsvD2wn8CRj3",
        "colab_type": "text"
      },
      "source": [
        "## 2.1 강화학습 개요\n",
        "- **강화학습** : 원하는 목표를 달성하기 위해 시간 순서대로 시스템에 가해지는 행동(action)을 선택하기 위한 방법.\n",
        "\n",
        "  - 구성 요소 : 에이전트, 환경, 에이전트와 환경과의 상호작용 행동, 보상, 상태의 관측\n",
        "  - 시간 변수는 불연속적이며 모든 시간스텝마다 행동이 가해진다.\n",
        "  - 의사 결정자 : 에이전트(agent)\n",
        "  - 시스템 : 에이전트의 환경(environment)\n",
        "  - agent는 환경의 변화를 표현하는 상태(state)을 관측(dbservation)하여 일정한 정책(policy)하에 불연속적인 값이나 연속적인 값으로 된 행동을 선택하여, 이를 환경에 인가해 환경을 변화시기킨다. 그 결과 시간스텝마다 의사 결정 성과를 평가하는 보상(reward)를 제공받는다.\n",
        "  - 강화학습의 목표 : **종료되는 시점까지 누적된 총 보상을 최대화하는 것.**\n",
        "  - 순차적인 의사 결정의 특징\n",
        "\n",
        "    : 일정 시간스텝에서 일시적으로 보상이 작더라도 누적 보상을 최대로 할 수 있는 행동을 선택.\n",
        "   \n",
        "- 예시 1 : 그리드 월드\n",
        "\n",
        "![대체 텍스트](https://github.com/syeong1218/RL/blob/master/2-1.jpg?raw=true)\n",
        "\n",
        "목표 : 로봇이 (4,3)으로 스스로 찾아가게 해서 최대 보상을 받는 것\n",
        "\n",
        "에이전트 : 이동방향을 결정하는 로본의 컴퓨터\n",
        "\n",
        "환경 : 에이전트를 제외한 로봇과 그리드 월드 전체\n",
        "\n",
        "상태 : 그리드 월드 내에서 로봇의 위치\n",
        "\n",
        "행동 : 상(U), 하(D), 좌(L), 우(R)\n",
        "\n",
        "=> 로봇의 위치를 바꿔가며 받는 보상을 통해 시행착오를 겪으면서 R-R-U-U-R,  R-R-R-L-U-U-L-L-R-R-R과 같은 행동을 선택한다. 로봇이 이동할 때마다 보상 -0.01을 받는다고 하면  R-R-U-U-R이나 U-U-R-R-R 중 하나를 선택 할 것이다.\n",
        "\n",
        "- 최적 제어와 강호학습의 차이점\n",
        "\n",
        " : **제어 대상 시스템의 수학적 동역학 모델의 유무**\n",
        "    - 최적제어 : 수학적 모델을 가지고있어 시스템이 어떻게 동작할지 예측 가능하다.\n",
        "    - 강화학습 : 수학적 모델을 요구하지 않고 시스템에서 얻은 데이터만 사용한다.\n",
        "\n",
        "    => 다양한 변수들로 인해 시스템의 모델을 전혀 얻을 수 없는 경우가 있는데, 이 경우에는 강화학습이 좋다.\n",
        "\n",
        "    - 예시 : 드론이 장애물을 피해가면서 최소의 에너지를 사용해 목표 지점에 도달하도록 드론에 가해지는 제어 입력 또는 행동을 순차적으로 결정하는 것이 목표.\n",
        "\n",
        "![대체 텍스트](https://github.com/syeong1218/RL/blob/master/2-3.jpg?raw=true)\n",
        "\n",
        "| |최적제어|강화학습|\n",
        "|----|----|----|\n",
        "|비행 제어 컴퓨터|제어기|에이전트|\n",
        "|컴퓨터를 제외한 모든 것|시스템|환경|\n",
        "||수학적 모델에 의존|보상에 의존|\n",
        "\n",
        "![대체 텍스트](https://github.com/syeong1218/RL/blob/master/2-4.jpg?raw=true)\n",
        "=> 최적 제어는 비행 제어 컴퓨터를 제외한 모든 환경에 대한 수학적 모델을 알고 있고, 강화학습은 아무것도 모른다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kFa6UBGCUg7",
        "colab_type": "text"
      },
      "source": [
        "## 2.2 강화학습 프로세스와 표기법\n",
        "\n",
        "![대체 텍스트](https://github.com/syeong1218/RL/blob/master/2-5.jpg?raw=true)\n",
        "\n",
        "(1) 에이전트는 환경의 상태($x_t$)를 측정한다.\n",
        "\n",
        "(2) 측정한 상태에서 에이전트의 정책으로 선택한 행동($u_t$)을 환경에 보낸다.\n",
        "\n",
        "    정책 : 측정한 상태를 바탕으로 최선의 행동으 ㄹ선택하기 위한 에이전트의 규칙 또는 방법\n",
        "\n",
        "(3) 행동에 의해서 환경의 상태는 다음 상태($x_{t+1}$)로 전환된다.\n",
        "\n",
        "(4) 전환된 환경의 상태를 바탕으로 다시 에이전트는 새로운 행동을 선택한다.\n",
        "\n",
        " (5) 환경으로부터 주어지는 즉각적인 보상($r_{t+1}$)을 사용해 장기적인 성과를 계산 또는 예측해서 에이전트의 정책을 개선한다.\n",
        "\n",
        " => 이와 같이 시간스텝마다 에이전트가 새로운 보상을 받는 것을 반복하는 과정을 수학적으로 모델링한 것이 마르코프 결정 프로세스(MDP)이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiqYq0_HFCfA",
        "colab_type": "text"
      },
      "source": [
        "# 2.3 마르코프 결정 프로세스"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NbGXMQIFHOJ",
        "colab_type": "text"
      },
      "source": [
        "## 2.3.1 정의"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KT4zoBTFFHKr",
        "colab_type": "text"
      },
      "source": [
        "### **마르코프 결정 프로세스(MDP)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw_uvuefpGGY",
        "colab_type": "text"
      },
      "source": [
        " : 상태($X_t$), 상태전이 확률밀도함수($p$), 행동($U_t$), 보상함수($r(X_t,U_t)$)로 이루어진 이산시간 확률 프로세스"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQHuZWrOGtBK",
        "colab_type": "text"
      },
      "source": [
        "-> 이산시간에서 시간스텝마다 에이전트가 환경의 상태를 측정 -> 적절한 행동 -> 그 다음상태로 전환 -> 보상 -> 다시 새로운 상태"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BN97EehRumlq",
        "colab_type": "text"
      },
      "source": [
        "##2.3.1.1 확률적 MDP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1wJHC09urFU",
        "colab_type": "text"
      },
      "source": [
        "$\\rightarrow$  환경 모델이 상태전이 확률밀도함수로 주어지는 경우 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhLYQ85XFHHQ",
        "colab_type": "text"
      },
      "source": [
        "### **상태전이 확률밀도함수**($p$) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2VaKdyfpKIR",
        "colab_type": "text"
      },
      "source": [
        ": 어떤 상태($X_t$)에서 에이전트가 행동($U_t$)을 했을 떄, 다음 상태($X_{t+1}$)로 갈 확률밀도함수"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba6o_QiYFHDk",
        "colab_type": "text"
      },
      "source": [
        "연속공간 상태 및 행동변수 -> $p(X_{t+1} \\mid X_t, U_t) $\n",
        "<br> 이산공간 상태 및 행동변수 -> $P(X_{t+1} \\mid X_t, U_t) $"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvDpowdkFG_o",
        "colab_type": "text"
      },
      "source": [
        "이 함수는 미래의 상태(다음의 상태)가 과거의 상태와 행동에 관계없이 오직 현재 상태와 행동에만 영향을 받는다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bM-asRcNFG7L",
        "colab_type": "text"
      },
      "source": [
        "다른말로 **마르코프 시퀀스** 라고 하며, $p(X_{t+1} \\mid X_t,X_{t-1},\\ldots,X_0, U_t,U_{t-1} U_0) = p(X_{t+1} \\mid X_t, U_t) $ 이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-o-BAQdo5-G",
        "colab_type": "text"
      },
      "source": [
        "### **조건부 확률밀도함수($\\pi$)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T76zrtpSow80",
        "colab_type": "text"
      },
      "source": [
        ": MDP에서 **누적된 보상**을 **가장 많이 획득**하기 위해 각 상태에서 **어떤 행동을 취할 것인지**를 나타내는 함수"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-jBTRybow5V",
        "colab_type": "text"
      },
      "source": [
        "$$\\pi(U_t \\mid X_t) =p(U_t \\mid X_t)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbjWnr2oow3A",
        "colab_type": "text"
      },
      "source": [
        "$\\pi(U_t \\mid X_t)$ 를 **정책** 이라고 하며, **한 상태변수**에서 **여러개의 행동**을 **선택**할 수 있는 가능성이 있다는 것을 알 수 있다. \n",
        "<BR>다른말로 **확률적 정책**이라고도 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqyOxDYdow0Y",
        "colab_type": "text"
      },
      "source": [
        "![image](https://user-images.githubusercontent.com/53015968/81427077-2aa78e00-9195-11ea-8317-5a97c44db091.png)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pC3J6W47owxd",
        "colab_type": "text"
      },
      "source": [
        "위 그림은 상태변수 $X_0$에서 어떤 정책 $\\pi$에 의해 행동 $U_0$가 확률적으로  선택되면, 상태전이 확률$P(X_{1}\\mid X_0,U_0)$에 의해 $X_1$으로 이동하고 이때, 보상 $r(X_0,U_0)$이 주어지며 이 과정이 반복되는 MDP 전개 그림이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXFhlYs4owud",
        "colab_type": "text"
      },
      "source": [
        "**궤적($\\tau$)** : **상태변수**와 **행동**의 **연속**적인 **시퀀스**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67E2pk2Itsmc",
        "colab_type": "text"
      },
      "source": [
        "$$\\tau = (X_0,U_0,X_1,U_1,\\ldots,X_\\tau,U_\\tau)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuYiQ5n3u3Bq",
        "colab_type": "text"
      },
      "source": [
        "## 2.3.1.2 확정적 MDP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iN8SPrHt_hS",
        "colab_type": "text"
      },
      "source": [
        "$\\rightarrow$ 환경모델과 정책이 모두 확정적으로 주어지는 경우"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwnNlL7-vL7l",
        "colab_type": "text"
      },
      "source": [
        "**환경 모델** :  $X_{t+1}=f(X_t,U_t)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUCQlFxuuT18",
        "colab_type": "text"
      },
      "source": [
        "이는, 시간스텝 $t$에서 상태와 행동이 주어지면 다음상태를 확정적으로 알 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDrkgRQfuT4-",
        "colab_type": "text"
      },
      "source": [
        "**정책** : $U_t=\\pi(X_t)$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrnsGhWLuTy5",
        "colab_type": "text"
      },
      "source": [
        "정책은 상태변수에서 행동을 **직접 계산**한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3jxt9TfuTv_",
        "colab_type": "text"
      },
      "source": [
        "### **반환값($G_t$)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cs2OnG_2uTtF",
        "colab_type": "text"
      },
      "source": [
        "$\\rightarrow$ 시간스텝 $t$일 때, 미래에 얻을 수 있는 **보상의 총합**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76Ym3_U1uTpI",
        "colab_type": "text"
      },
      "source": [
        "$$G_t=\\sum_{k=t}^T  \\gamma^{k-t}r(X_k,U_k)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFsU-AW8uTmM",
        "colab_type": "text"
      },
      "source": [
        "$\\gamma \\in [0,1]$은 **감가율**이다. 값이 작을수록 보상을 가까운 미래에 받는다. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrEi5fdYuTjw",
        "colab_type": "text"
      },
      "source": [
        "### **에피소드**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t87CTO45uThU",
        "colab_type": "text"
      },
      "source": [
        ": 어떤 정책을 시행해 $X_0 \\rightarrow U_0 \\rightarrow r(X_0,U_0)\\rightarrow \\ldots\\rightarrow X_T\\rightarrow U_T$의 순서로 전개될 때, 이러한 **상태변수, 행동, 보상의 시퀀스 집합**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ua6EdqpnuTd5",
        "colab_type": "text"
      },
      "source": [
        "유한 구간 에피소드 : 특정 상태변수에 도달하는 등 목적이 성취되면 종료되는 에피소드($t=T$)\n",
        "<BR>무한 구간 에피소드 : $T=\\infty$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ek8Ok8WuTYz",
        "colab_type": "text"
      },
      "source": [
        "## 2.3.2 가치함수 - 상태가치 & 행동가치"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkaLffaouTVw",
        "colab_type": "text"
      },
      "source": [
        "##2.3.2.1 상태가치"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdHHpkS91BPZ",
        "colab_type": "text"
      },
      "source": [
        "![image](https://user-images.githubusercontent.com/53015968/81427186-66425800-9195-11ea-9d18-f9c1169d1fae.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TT2T9BJF0opw",
        "colab_type": "text"
      },
      "source": [
        "$\\rightarrow$ 상태변수($X_t$)에서 정책 $\\pi$에 의해 **행동이 가해졌을 때**, 기대할 수 있는 **반환값**.  즉, 미래 보상의 총합"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDPyV2AN1ZUx",
        "colab_type": "text"
      },
      "source": [
        "상태가치함수 식 : $V^\\pi(X_t)=E_{\\tau_{u_t:u_T \\sim p(\\tau_{u_t:u_T\\mid X_t})}}[\\sum_{k=t}^T \\gamma^{k-t}r(X_k,u_k)\\mid X_t]$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxfuzwIp1v6v",
        "colab_type": "text"
      },
      "source": [
        "#### 식의 인자의 의미"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su4kZiGP1bCw",
        "colab_type": "text"
      },
      "source": [
        "$p(\\tau_{u_t:u_T}\\mid X_t)$ $\\Rightarrow$  기댓값 계산 시, 확률밀도함수로 조건부 확률밀도함수 사용\n",
        "<br>$\\tau_{u_t:u_T} = (u_t,X_{t+1},u_{t+1},\\ldots,X_T,u_T)$ $\\Rightarrow$ 상태변수$X_t$에서 정책으로 생성되는 궤적"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5A09CoowuTTI",
        "colab_type": "text"
      },
      "source": [
        "## 2.3.2.2 행동가치"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNYRMFgb3Sde",
        "colab_type": "text"
      },
      "source": [
        "![image](https://user-images.githubusercontent.com/53015968/81427917-6d1d9a80-9196-11ea-97ea-f00797babb79.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucMYcvvd3T_R",
        "colab_type": "text"
      },
      "source": [
        "$\\rightarrow$ 상태변수($X_t$)에서  **행동($u_t$)을 선택하고**, 그로부터 정책($\\pi$)에 의해 행동이 가해졌을 때,기대할 수 있는 **반환값**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_YnhiHN3xwF",
        "colab_type": "text"
      },
      "source": [
        "행동가치 식 :  $Q^\\pi(X_t,u_t)=E_{\\tau_{x_{t+1}:u_T \\sim p(\\tau_{u_{t+1}:u_T\\mid X_t,u_t})}}[\\sum_{k=t}^T \\gamma^{k-t}r(X_k,u_k)\\mid X_t,u_t]$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nX6-7Fn30KL",
        "colab_type": "text"
      },
      "source": [
        "#### 식의 인자의 의미"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwhtO1lV30hu",
        "colab_type": "text"
      },
      "source": [
        "$p(\\tau_{u_{t+1}:u_T}\\mid X_t)$ $\\Rightarrow$  기댓값 계산 시, 확률밀도함수로 조건부 확률밀도함수 사용\n",
        "<br>$\\tau_{u_{t+1}:u_T} = (X_{t+1},u_{t+1},\\ldots,X_T,u_T)$ $\\Rightarrow$ 상태변수$X_t$에서 행동$u_t$를 선택하고 정책으로 생성되는 궤적"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtOEgeko30fI",
        "colab_type": "text"
      },
      "source": [
        "이 두 식을 이용하면 다음과 같은 결론을 얻을 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NnxxRh230cw",
        "colab_type": "text"
      },
      "source": [
        "$V^\\pi(X_t)=E_{u_t \\sim \\pi(u_t \\mid X_t)}[Q^\\pi (X_t,u_t)]$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ap4ayAWY30aP",
        "colab_type": "text"
      },
      "source": [
        "즉, **상태가치**는 상태변수$X_t$에서 **선택 가능한 모든 행동**$u_t$에 대한 **행동가치의 평균값**이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHqXnNtM30GD",
        "colab_type": "text"
      },
      "source": [
        "## 2.3.3 벨만 방정식"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFyaLHQL441I",
        "colab_type": "text"
      },
      "source": [
        ": **현재** 상태변수의 **가치**와 **다음** 시간스텝의 상태변수의 **가치**와의 관게"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeZhytoa45Qb",
        "colab_type": "text"
      },
      "source": [
        "$V^\\pi(X_t)=E_{u_t \\sim \\pi(u_t \\mid X_t)}[r(X_t,u_t)+E_{x_{t+1} \\sim p(X_{t+1} \\mid X_t,u_t)}[rV^\\pi(X_{t+1})]]$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCLX91nE45N4",
        "colab_type": "text"
      },
      "source": [
        "여기서 상태가치 함수($V^\\pi(X_t), V^\\pi (X_{t+1})$)는 원래 시변함수이므로 두 상태가치 함수는 다르다.\n",
        "<br>명확히 하기위해 위와 같이 명백히 표시해야한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alQOGakuAbhP",
        "colab_type": "text"
      },
      "source": [
        "하지만, 편의상  $T \\rightarrow \\infty$ (무한구간)이라면 두 상태 함수는 시불변함수가 되어 동일하게 된다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAIIErh045LY",
        "colab_type": "text"
      },
      "source": [
        "## 2.3.4 벨만 최적 방정식"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffMyIMQzBXDl",
        "colab_type": "text"
      },
      "source": [
        ": **현재의 최적 가치**와 **다음** 시간스텝의 **최적 가치**와의 관계"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FC_z6YyVCWhI",
        "colab_type": "text"
      },
      "source": [
        "**최적 정책**($\\pi^*(u_t \\mid X_t)$) : 상태가치의 값을 최대로 만드는 **정책**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQPSsUxJ45I_",
        "colab_type": "text"
      },
      "source": [
        "**최적 상태가치 함수** : 모든 정책 중 상태가치 값을 최대로 만드는 정책을 적용한 상태가치 함수\n",
        "<br>**최적 행동가치 함수** : 모든 정책 중 행동가치 값을 최대로 만드는 정책을 적용한 행동가치 함수"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxAXPC3BuTQQ",
        "colab_type": "text"
      },
      "source": [
        "최적 상태가치 함수는 최적 행동가치 함수의 최댓값을 취하는 아래와 같은 관계를 갖는다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pcEFNFQCC-B",
        "colab_type": "text"
      },
      "source": [
        "$V^*(X_t)=maxQ^*(X_t,u_t)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMsk5m_oCtNH",
        "colab_type": "text"
      },
      "source": [
        "##2.4 강화학습 방법"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luAOwyjBDhuS",
        "colab_type": "text"
      },
      "source": [
        "1. 가치함수 추정\n",
        "2. 직접 정책 유도\n",
        "3. 환경 모델 추정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8rFqzBZC68Y",
        "colab_type": "text"
      },
      "source": [
        "### 강화학습의 반복 과정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lp7_DK7zDseP",
        "colab_type": "text"
      },
      "source": [
        "위 세가지 방법은 아래와 같은 세가지의 공통점을 가진다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAL2uidkC_vN",
        "colab_type": "text"
      },
      "source": [
        "![2 9](https://user-images.githubusercontent.com/53015968/81430084-d05cfc00-9199-11ea-9deb-00c641339c66.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyDUe99VDA5r",
        "colab_type": "text"
      },
      "source": [
        "1. 정책 실행을 통한 데이터 생성\n",
        "2. 모델 또는 가치함수의 추정\n",
        "3. 정책 개선 \n",
        "<br>1 ~ 3단계 반복 $\\rightarrow$ 최적의 정책 산출"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVeFmkX5DHjx",
        "colab_type": "text"
      },
      "source": [
        "### 2.4.1 가치함수 추정 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SD_xCXcIEE6w",
        "colab_type": "text"
      },
      "source": [
        "$\\rightarrow$ **가치함수를 추정**하여 **최대의 보상**을 계산"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0B513XOEhKJ",
        "colab_type": "text"
      },
      "source": [
        "#### 가치 기반 강화학습 방법"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8H40ygIFBAC",
        "colab_type": "text"
      },
      "source": [
        "![2 10](https://user-images.githubusercontent.com/53015968/81430090-d226bf80-9199-11ea-8652-e6b5b5dce33c.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnulGV_IEMzJ",
        "colab_type": "text"
      },
      "source": [
        "1. 행동가치 함수를 추정\n",
        "2. 각 상태에서 행동가치 함수를 최대화하는 행동 선택\n",
        "3. 최적 정책 유도"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SSa7lBbD_gb",
        "colab_type": "text"
      },
      "source": [
        "대표적인 예 : DQN(deoop Q network) - 이산공간의 상태 & 행동 기반"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Z9_VEAzEx-D",
        "colab_type": "text"
      },
      "source": [
        "### 2.4.2 직접 정책 유도"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79qbxMTdFUX_",
        "colab_type": "text"
      },
      "source": [
        "목적 :  **보상의 기댓값을 최대**로 하기 위해 **정책($\\pi_\\theta (u_t \\mid X_t)$)**을 최적화하는 **정책 파라미터** $\\theta$를 계산"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Rp6f50BFxit",
        "colab_type": "text"
      },
      "source": [
        "#### 정책 그래디언트 방법"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnFKJC93E3OG",
        "colab_type": "text"
      },
      "source": [
        "![2 11](https://user-images.githubusercontent.com/53015968/81430093-d357ec80-9199-11ea-9e30-d9dc1fda779a.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzJtLcyBF1FM",
        "colab_type": "text"
      },
      "source": [
        "위 방법은 보통 가치함수도 함께 추정하여 정책의 성과를 평가하는 **Actor-Critic** 구조를 사용한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15R1F6RDG4jN",
        "colab_type": "text"
      },
      "source": [
        "+  Actor-Critic : 강화학습의 MDP 중 정책과 가치함수를 Deep Neural Network로 parameterization(파라미터를 이용해 전체를 예측)하여 근사화한 기법"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8sSGo5-FEan",
        "colab_type": "text"
      },
      "source": [
        "### 2.4.3 환경 모델 추정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4Y9kxdRHyys",
        "colab_type": "text"
      },
      "source": [
        "#### 모델 기반 강화학습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsG2TVEvIFlL",
        "colab_type": "text"
      },
      "source": [
        "![2 12](https://user-images.githubusercontent.com/53015968/81430098-d4891980-9199-11ea-83e2-6cd439c72da4.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxXCROjiH1ec",
        "colab_type": "text"
      },
      "source": [
        "모델 기반 강화학습은 간단하고 효율적이다. 또한 사용되는 정책 개선은 환경 모델을 정의하는 방식에 따라 다르다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dY4GKlEHokaD",
        "colab_type": "text"
      },
      "source": [
        "# 3장 정책 그래디언트  \n",
        "\n",
        "## 3.1 배경  \n",
        "강화학습 문제를 해결하는 모든 방법의 최종 목표는 노적 보상을 최대화하는 최적 정책(Optimal policy)을 구하는 것이다.  \n",
        "이 장에서는 정책(policy)를 파라미터화하고, 누적 보상을 파라미터화된 정책의 함수로 기술하여 누적 보상과 정책 파라미터 간의 관계를 구축하고 최적화 방법을 통해 누적 보상을 최대화하는 정책 파라미터를 계산하는 강화학습의 방법론 중 정책 그래디언트를 알아볼 것이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zC-_xIFhoqrI",
        "colab_type": "text"
      },
      "source": [
        "## 3.2 목적함수  \n",
        "강화학습의 목표를 수학적으로 표현하면 반환값의 기댓값으로 이루어진 목적함수 __J__를 최대로 만드는 정책 $\\pi(u_t|x_t)$을 구하는 것이다.  \n",
        "정책이 $\\theta$로 파라미터화 됐다면 $\\pi_\\theta(u_t|x_t)$로 표현하고 목적함수를 최대로 만드는 정책 파라미터 $\\theta$를 계한하는 것이라 할 수 있다.  \n",
        "$$\\theta^*=argmaxJ(\\theta)$$\n",
        "$$J(\\theta)=E_{\\tau \\sim p_\\theta(\\tau)}[\\sum_{t=0}^T \\gamma^t r(x_t,u_t)] $$  \n",
        "여기서 $r(x_t, u_t)$는 시간스텝 $t$일 때 상태변수가 $x_t$에서 행동 $u_t$를 사용했을 때 에이전트가 받는 보상을 나타내며, $\\gamma\\in[0,1]$은 감가율(discount factor)이다. 기댓값의 아래 첨자 $p_\\theta(\\tau)$는 기댓값을 계산할 때 확률밀도함수로 $p_\\theta(\\tau)$를 사용한다는 의미다.  \n",
        "그림 3.1과 같이 $\\tau$는 정책 $\\pi_\\theta$로 생성되는 궤적 $\\tau=(x_0, u_0, x_1, u_1, x_2, u_2, x_3, u_3, \\cdots, x_T, u_T)$이다. $p_\\theta(\\tau)$는 정책 $\\pi_\\theta$로 생성되는 궤적의 확률밀도함수를 나타낸다.\n",
        "\n",
        "![KakaoTalk_20200509_145820465](https://user-images.githubusercontent.com/53211502/81465554-c4a71f00-9205-11ea-852f-4aa6c861bb7b.jpg)  \n",
        "\n",
        "정책은 보통 신경망(neural network)으로 파라미터화된다. 이 신겸망을 정책 신경망(policy neural network)이라고 하며, 파라미터 $\\theta$는 그림 3.2에 도시한 것 같이 신경망의 모든 가중치다.  \n",
        "![KakaoTalk_20200509_151311299](https://user-images.githubusercontent.com/53211502/81465828-f0c39f80-9207-11ea-968a-db245cc2df55.jpg)\n",
        "\n",
        "에이전트의 정책 신경망과 환경과의 상호작용을 마르코프 결정 프로세스(MDP) 프레임워크로 표현하면 다음 그림 3.3과 같다.\n",
        "![KakaoTalk_20200509_152306979](https://user-images.githubusercontent.com/53211502/81465989-0e453900-9209-11ea-96c8-92d6c716625e.jpg)\n",
        "\n",
        "$G_0 = \\sum_{t=0} ^T \\gamma^t r (x_t, u_t)$는 시간 스텝 $t=0$부터 에피소드가 종료될 때까지 받을 수 있는 전체 궤적에 대한 감가 보상(discount reward)의 총합으로서, 전체 반환값 $G_0$라고 부른다. 임의의 시간 $k=t$부터 에피소드가 종료될 때까지 받을 수 있는 예정 보상(reward-to-go)의 총합은 $G_k = \\sum_{k=t}^T \\gamma^{k-t} r (x_k, u_k)$로 표시한다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wO36mh2npLMp",
        "colab_type": "text"
      },
      "source": [
        "$p_\\theta (\\tau) = p_\\theta(x_0, u_0, x_1, u_1, \\cdots. x_T, u_T)$  \n",
        "전개된 $p_\\theta (\\tau)$를 베이즈 정리와 마르코프 시퀀스 가정에 의해 다음과 같이 표현할 수 있다.  \n",
        "$$p_\\theta (\\tau) = p(x_0) \\prod_{t=0}^T \\pi_\\theta(u_t|x_t)p(x_{t+1}|x_t, u_t)$$  \n",
        "\n",
        "*  베이즈 정리  \n",
        "$$P\\{A\\} = \\sum_{i=1}^n P\\{A, B_i\\} = \\sum_{i=1}^n P\\{A|B_i\\} P\\{B_i\\}$$  \n",
        "* 마르코프 시퀀스 가정  \n",
        " 현재의 state가 history의 모든 관련 정보를 갖고 있다.  \n",
        "\n",
        "목적함수 $J$를 전개하면 다음과 같다.  \n",
        "$$J(\\theta)=E_{\\tau \\sim p_\\theta(\\tau)}[\\sum_{t=0}^T \\gamma^t r(x_t,u_t)] \n",
        "           =\\int_\\tau p_\\theta(\\tau)(\\sum_{t=0} ^T \\gamma^t r (x_t, u_t))d\\tau $$  \n",
        "여기서 궤적을 $x_0$과 나머지, 두 영역으로 분할하고 베이즈 정리를 적용하면 다음과 같다.  \n",
        "$$p_\\theta (\\tau) = p(x_0) p_\\theta(\\tau_{u_0 : u_T}|x_0)$$\n",
        "위 식을 목적함수 전개 식에 대입 후 상태가치 함수 $V^{\\pi_\\theta}$로 묶어 주면 목적함수는 다음과 같다.  \n",
        "$$J(\\theta) = E_{x_0 \\sim p(x_0)}[V^{\\pi_\\theta}(x_0)]$$  \n",
        "즉 목적함수는 초기 상태변수 $x_0$에 대한 상태가치의 평균값이 된다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFraaedHot6z",
        "colab_type": "text"
      },
      "source": [
        "## 3.3 정책 그래디언트  \n",
        "목적함수를 최대로 만드는 $\\theta$를 계산하기 위해 목적함수를 $\\theta$로 미분해 보자.  \n",
        "$$ {{\\partial J(\\theta)}\\over{\\partial \\theta}}= \\nabla_\\theta J(\\theta) = \\int_\\tau \\nabla_\\theta p_\\theta(\\tau) \\sum_{t=0}^T \\gamma^t r (x_t, u_t) d\\tau=\\int_\\tau p_\\theta(\\tau) \\nabla_\\theta \\log p_\\theta(\\tau) \\sum_{t=0}^T \\gamma^t r (x_t, u_t) d\\tau$$  위 식을 목적함수의 그래디언트 식이라 한다.\n",
        "$\\nabla_\\theta \\log p_\\theta(\\tau) = {{\\nabla_\\theta p_\\theta(\\tau)}\\over{p_\\theta(\\tau)}}$를 이용하여 log를 전개하면 다음과 같다.  \n",
        "$$\\nabla_\\theta \\log p_\\theta(\\tau) = \\nabla_\\theta (\\log p(x_0) + \\sum_{t=0}^T \\log \\pi_\\theta (u_t|x_t) + \\log p(x_{t+1}|x_t, u_t))$$  \n",
        "좌변의 두번째 항만 $\\theta$의 함수이므로 $\\nabla_\\theta \\log p_\\theta(\\tau) =  \\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta (u_t|x_t)$이다. 이 식을 목적함수의 그래디언트 식에 대입하면 환경 모델이 필요 없는 모델 프리(model-free) 강화학습 방법이 되고 그 식은 다음과 같다.  \n",
        "$$\\nabla_\\theta J(\\theta)=E_{\\tau \\sim p_\\theta(\\tau)}[\\sum_{t=0}^T (\\nabla_\\theta \\log \\pi_\\theta (u_t|x_t)(\\sum_{k=0}^T\\gamma^k r (x_k, u_k)))] $$  \n",
        "위 식에 시간스텝 t<k에서의 보상값에 영향을 끼치지 못한다는 점(인과성)을 고려하고 무한 구간 에피소드에서도 적용 가능하게 하기 위해 예정 보상에만 감가율을 적용하면 다음과 같다.  \n",
        "$$\\nabla_\\theta J(\\theta)=E_{\\tau \\sim p_\\theta(\\tau)}[\\sum_{t=0}^T (\\nabla_\\theta \\log \\pi_\\theta (u_t|x_t)(\\sum_{k=t}^T\\gamma^{k-t} r (x_k, u_k)))] $$  \n",
        "목적함수를 최대로 하는 파라미터 $\\theta$는 다음과 같이 경사상승법으로 구할 수 있다.  \n",
        "$$\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta J(\\theta)$$  \n",
        "\n",
        "정책 그래디언트에 사용되는 목적함수 그래디언트를 다음과 같다.  \n",
        "* 목적함수 : $J(\\theta)=E_{\\tau \\sim p_\\theta(\\tau)}[\\sum_{t=0}^T \\gamma^t r(x_t,u_t)], \\tau=(x_0, u_0. x_1. u_1, \\cdots, x_T, u_T)$  \n",
        "* 가정 : 확률적 정책, $u_t \\sim \\pi_\\theta (u_t|x_t)$  \n",
        "* 그래디언트 : $\\nabla_\\theta J(\\theta)=E_{\\tau \\sim p_\\theta(\\tau)}[\\sum_{t=0}^T (\\nabla_\\theta \\log \\pi_\\theta (u_t|x_t)(\\sum_{k=t}^T\\gamma^{k-t} r (x_k, u_k)))] $\n",
        "* 업데이트 : $\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta J(\\theta)$  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrwGvtcco67P",
        "colab_type": "text"
      },
      "source": [
        "## 3.4 REINFORCE 알고리즘  \n",
        "정책 그래디언트를 실제 적용하는 데 있어서 $\\tau$상의 기댓값은 에피소드를 M개만큼 샘플링해 에피소드 평균을 이요해서 근사적으로 계산한다. 위 방법을 이용하면 목적함수의 그래디언트는 다음과 같이 근사적으로 추정할 수 있다.  \n",
        " $$\\nabla_\\theta J(\\theta) \\approx \\nabla_\\theta {1 \\over M} \\sum_{m=1}^M[\\sum_{t=0}^T ( \\log \\pi_\\theta (u_t^{(m)}|x_t^{(m)})G_t^{(m)})]$$  \n",
        "\n",
        " M개의 에피소드를 이용해 목적함수의 그래디언트를 계산하는 식을 도시하면 다음과 같다.  \n",
        " ![KakaoTalk_20200509_171445690](https://user-images.githubusercontent.com/53211502/81468127-a4cd2680-9218-11ea-9432-fa672b5d0661.jpg)\n",
        "\n",
        "한 개의 에피소드를 이용해 목적함수의 그래디언트를 계산하는 방법을 도시하면 다음과 같다.  \n",
        "![KakaoTalk_20200509_171808860](https://user-images.githubusercontent.com/53211502/81468209-202ed800-9219-11ea-8bc1-8fb3fdaa080b.jpg)\n",
        "\n",
        "파라미터 $\\theta$로 표현된 정책 $\\pi_\\theta$을 신경망으로 구성할 때  \n",
        "$\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta J(\\theta) \\approx \\alpha \\nabla_\\theta \\sum_{t=0}^T ( \\log \\pi_\\theta (u_t^{(m)}|x_t^{(m)})G_t^{(m)})$이기 때문에  \n",
        "에피소드의 손실함수로 $loss = -\\sum_{t=0}^T ( \\log \\pi_\\theta (u_t^{(m)}|x_t^{(m)})G_t^{(m)})$를 사용한다.  \n",
        "따라서 반환값을 크게 받은 정책의 에피소드는 목적함수의 그래디언트 계산 시 더 큰 영향을 끼치고 반환값이 작은 정책의 에피소드는 작은 영향을 끼쳐, 점진적으로 정책이 개선된다.  \n",
        "\n",
        "다음은 REINFORCE 알고리즘의 프로세스이다.  \n",
        "![KakaoTalk_20200509_173230345](https://user-images.githubusercontent.com/53211502/81468581-6be28100-921b-11ea-97c4-572781ac3eae.jpg)\n",
        "\n",
        "정책 $\\pi_{\\theta_1}$을 에피소드가 종료할 때까지 실행시켜 $(T_1+1)$개의 샘플 $(x_t, u_t, r(x_t, u_t), x_{t+1})_{t=0, \\dots, T_1}$을 생성하고, 이를 바탕으로 정책을 $\\pi_{\\theta_1}$에서 $\\pi_{\\theta_2}$로 업데이트한다. 그 후 $(T_1+1)$개의 샘플은 바로 폐기하고, 다시 업데이트된 정책 $\\pi_{\\theta_2}$를 다른 에피소드가 종료할 때까지 실행시켜 다시 $(T_2+1)$개의 새로운 샘플$(x_t, u_t, r(x_t, u_t), x_{t+1})_{t=0, \\dots, T_2}$를 생성한다. 그리고 정책$\\pi_{\\theta_2}$를 새로운 정책으로 업데이트한다. 이러한 과정을 일정 학습 성과에 도달할 때까지 반복하면서 학습을 진행한다.  \n",
        "\n",
        "REINFORCE 알고리즘에는 몇 가지 문제가 있다.  \n",
        "1. 한 에피소드가 끝나야 정책을 업데이트할 수 있다.  \n",
        "$\\rightarrow$ 신경망 학습 시간이 상당히 길어질 수 있다.  \n",
        "2. 그래디언트의 분산이 매우 크다.  \n",
        "$\\rightarrow$ 반환값이 에피소드마다 크게 차이가 남에 따라 목적함수의 그래디언트도 이에 비례하여 값이 들쭉날쭉하므로 신경망 학습에 시간이 많이 걸리고 학습이 전혀 안 될 수도 있다.  \n",
        "3. 온-플리시(on-policy) 방법이다.  \n",
        "$\\rightarrow$ 정책을 업데이트하기 위해서 해당 정책을 실행시켜 발생한 에피소드가 필요하므로 효율성이 매우 떨어진다.  \n",
        "\n",
        "하지만 REINFORCE 알고리즘은 다른 정책 그래디언트 기반 알고리즘의 기초가 된다는 점에서 그 의의가 있다."
      ]
    }
  ]
}