{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL_Ch2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNjIxiSmezcvGPtogWt+1p9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/syeong1218/RL/blob/master/RL_Ch2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISGqpSpuCMU2",
        "colab_type": "text"
      },
      "source": [
        "# 2장 강화학습 개념"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsvD2wn8CRj3",
        "colab_type": "text"
      },
      "source": [
        "## 2.1 강화학습 개요\n",
        "- **강화학습** : 원하는 목표를 달성하기 위해 시간 순서대로 시스템에 가해지는 행동(action)을 선택하기 위한 방법.\n",
        "\n",
        "  - 구성 요소 : 에이전트, 환경, 에이전트와 환경과의 상호작용 행동, 보상, 상태의 관측\n",
        "  - 시간 변수는 불연속적이며 모든 시간스텝마다 행동이 가해진다.\n",
        "  - 의사 결정자 : 에이전트(agent)\n",
        "  - 시스템 : 에이전트의 환경(environment)\n",
        "  - agent는 환경의 변화를 표현하는 상태(state)을 관측(dbservation)하여 일정한 정책(policy)하에 불연속적인 값이나 연속적인 값으로 된 행동을 선택하여, 이를 환경에 인가해 환경을 변화시기킨다. 그 결과 시간스텝마다 의사 결정 성과를 평가하는 보상(reward)를 제공받는다.\n",
        "  - 강화학습의 목표 : **종료되는 시점까지 누적된 총 보상을 최대화하는 것.**\n",
        "  - 순차적인 의사 결정의 특징\n",
        "\n",
        "    : 일정 시간스텝에서 일시적으로 보상이 작더라도 누적 보상을 최대로 할 수 있는 행동을 선택.\n",
        "   \n",
        "- 예시 1 : 그리드 월드\n",
        "\n",
        "![대체 텍스트](https://github.com/syeong1218/RL/blob/master/2-1.jpg?raw=true)\n",
        "\n",
        "목표 : 로봇이 (4,3)으로 스스로 찾아가게 해서 최대 보상을 받는 것\n",
        "\n",
        "에이전트 : 이동방향을 결정하는 로본의 컴퓨터\n",
        "\n",
        "환경 : 에이전트를 제외한 로봇과 그리드 월드 전체\n",
        "\n",
        "상태 : 그리드 월드 내에서 로봇의 위치\n",
        "\n",
        "행동 : 상(U), 하(D), 좌(L), 우(R)\n",
        "\n",
        "=> 로봇의 위치를 바꿔가며 받는 보상을 통해 시행착오를 겪으면서 R-R-U-U-R,  R-R-R-L-U-U-L-L-R-R-R과 같은 행동을 선택한다. 로봇이 이동할 때마다 보상 -0.01을 받는다고 하면  R-R-U-U-R이나 U-U-R-R-R 중 하나를 선택 할 것이다.\n",
        "\n",
        "- 최적 제어와 강호학습의 차이점\n",
        "\n",
        " : **제어 대상 시스템의 수학적 동역학 모델의 유무**\n",
        "    - 최적제어 : 수학적 모델을 가지고있어 시스템이 어떻게 동작할지 예측 가능하다.\n",
        "    - 강화학습 : 수학적 모델을 요구하지 않고 시스템에서 얻은 데이터만 사용한다.\n",
        "\n",
        "    => 다양한 변수들로 인해 시스템의 모델을 전혀 얻을 수 없는 경우가 있는데, 이 경우에는 강화학습이 좋다.\n",
        "\n",
        "    - 예시 : 드론이 장애물을 피해가면서 최소의 에너지를 사용해 목표 지점에 도달하도록 드론에 가해지는 제어 입력 또는 행동을 순차적으로 결정하는 것이 목표.\n",
        "\n",
        "![대체 텍스트](https://github.com/syeong1218/RL/blob/master/2-3.jpg?raw=true)\n",
        "\n",
        "| |최적제어|강화학습|\n",
        "|----|----|----|\n",
        "|비행 제어 컴퓨터|제어기|에이전트|\n",
        "|컴퓨터를 제외한 모든 것|시스템|환경|\n",
        "||수학적 모델에 의존|보상에 의존|\n",
        "\n",
        "![대체 텍스트](https://github.com/syeong1218/RL/blob/master/2-4.jpg?raw=true)\n",
        "=> 최적 제어는 비행 제어 컴퓨터를 제외한 모든 환경에 대한 수학적 모델을 알고 있고, 강화학습은 아무것도 모른다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kFa6UBGCUg7",
        "colab_type": "text"
      },
      "source": [
        "## 2.2 강화학습 프로세스와 표기법\n",
        "\n",
        "![대체 텍스트](https://github.com/syeong1218/RL/blob/master/2-5.jpg?raw=true)\n",
        "\n",
        "(1) 에이전트는 환경의 상태($x_t$)를 측정한다.\n",
        "\n",
        "(2) 측정한 상태에서 에이전트의 정책으로 선택한 행동($u_t$)을 환경에 보낸다.\n",
        "\n",
        "    정책 : 측정한 상태를 바탕으로 최선의 행동으 ㄹ선택하기 위한 에이전트의 규칙 또는 방법\n",
        "\n",
        "(3) 행동에 의해서 환경의 상태는 다음 상태($x_{t+1}$)로 전환된다.\n",
        "\n",
        "(4) 전환된 환경의 상태를 바탕으로 다시 에이전트는 새로운 행동을 선택한다.\n",
        "\n",
        " (5) 환경으로부터 주어지는 즉각적인 보상($r_{t+1}$)을 사용해 장기적인 성과를 계산 또는 예측해서 에이전트의 정책을 개선한다.\n",
        "\n",
        " => 이와 같이 시간스텝마다 에이전트가 새로운 보상을 받는 것을 반복하는 과정을 수학적으로 모델링한 것이 마르코프 결정 프로세스(MDP)이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiqYq0_HFCfA",
        "colab_type": "text"
      },
      "source": [
        "# 2.3 마르코프 결정 프로세스"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NbGXMQIFHOJ",
        "colab_type": "text"
      },
      "source": [
        "## 2.3.1 정의"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KT4zoBTFFHKr",
        "colab_type": "text"
      },
      "source": [
        "### **마르코프 결정 프로세스(MDP)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw_uvuefpGGY",
        "colab_type": "text"
      },
      "source": [
        " : 상태($X_t$), 상태전이 확률밀도함수($p$), 행동($U_t$), 보상함수($r(X_t,U_t)$)로 이루어진 이산시간 확률 프로세스"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQHuZWrOGtBK",
        "colab_type": "text"
      },
      "source": [
        "-> 이산시간에서 시간스텝마다 에이전트가 환경의 상태를 측정 -> 적절한 행동 -> 그 다음상태로 전환 -> 보상 -> 다시 새로운 상태"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BN97EehRumlq",
        "colab_type": "text"
      },
      "source": [
        "##2.3.1.1 확률적 MDP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1wJHC09urFU",
        "colab_type": "text"
      },
      "source": [
        "$\\rightarrow$  환경 모델이 상태전이 확률밀도함수로 주어지는 경우 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhLYQ85XFHHQ",
        "colab_type": "text"
      },
      "source": [
        "### **상태전이 확률밀도함수**($p$) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2VaKdyfpKIR",
        "colab_type": "text"
      },
      "source": [
        ": 어떤 상태($X_t$)에서 에이전트가 행동($U_t$)을 했을 떄, 다음 상태($X_{t+1}$)로 갈 확률밀도함수"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba6o_QiYFHDk",
        "colab_type": "text"
      },
      "source": [
        "연속공간 상태 및 행동변수 -> $p(X_{t+1} \\mid X_t, U_t) $\n",
        "<br> 이산공간 상태 및 행동변수 -> $P(X_{t+1} \\mid X_t, U_t) $"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvDpowdkFG_o",
        "colab_type": "text"
      },
      "source": [
        "이 함수는 미래의 상태(다음의 상태)가 과거의 상태와 행동에 관계없이 오직 현재 상태와 행동에만 영향을 받는다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bM-asRcNFG7L",
        "colab_type": "text"
      },
      "source": [
        "다른말로 **마르코프 시퀀스** 라고 하며, $p(X_{t+1} \\mid X_t,X_{t-1},\\ldots,X_0, U_t,U_{t-1} U_0) = p(X_{t+1} \\mid X_t, U_t) $ 이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-o-BAQdo5-G",
        "colab_type": "text"
      },
      "source": [
        "### **조건부 확률밀도함수($\\pi$)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T76zrtpSow80",
        "colab_type": "text"
      },
      "source": [
        ": MDP에서 **누적된 보상**을 **가장 많이 획득**하기 위해 각 상태에서 **어떤 행동을 취할 것인지**를 나타내는 함수"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-jBTRybow5V",
        "colab_type": "text"
      },
      "source": [
        "$$\\pi(U_t \\mid X_t) =p(U_t \\mid X_t)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbjWnr2oow3A",
        "colab_type": "text"
      },
      "source": [
        "$\\pi(U_t \\mid X_t)$ 를 **정책** 이라고 하며, **한 상태변수**에서 **여러개의 행동**을 **선택**할 수 있는 가능성이 있다는 것을 알 수 있다. \n",
        "<BR>다른말로 **확률적 정책**이라고도 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqyOxDYdow0Y",
        "colab_type": "text"
      },
      "source": [
        "![image](https://user-images.githubusercontent.com/53015968/81427077-2aa78e00-9195-11ea-8317-5a97c44db091.png)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pC3J6W47owxd",
        "colab_type": "text"
      },
      "source": [
        "위 그림은 상태변수 $X_0$에서 어떤 정책 $\\pi$에 의해 행동 $U_0$가 확률적으로  선택되면, 상태전이 확률$P(X_{1}\\mid X_0,U_0)$에 의해 $X_1$으로 이동하고 이때, 보상 $r(X_0,U_0)$이 주어지며 이 과정이 반복되는 MDP 전개 그림이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXFhlYs4owud",
        "colab_type": "text"
      },
      "source": [
        "**궤적($\\tau$)** : **상태변수**와 **행동**의 **연속**적인 **시퀀스**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67E2pk2Itsmc",
        "colab_type": "text"
      },
      "source": [
        "$$\\tau = (X_0,U_0,X_1,U_1,\\ldots,X_\\tau,U_\\tau)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuYiQ5n3u3Bq",
        "colab_type": "text"
      },
      "source": [
        "## 2.3.1.2 확정적 MDP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iN8SPrHt_hS",
        "colab_type": "text"
      },
      "source": [
        "$\\rightarrow$ 환경모델과 정책이 모두 확정적으로 주어지는 경우"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwnNlL7-vL7l",
        "colab_type": "text"
      },
      "source": [
        "**환경 모델** :  $X_{t+1}=f(X_t,U_t)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUCQlFxuuT18",
        "colab_type": "text"
      },
      "source": [
        "이는, 시간스텝 $t$에서 상태와 행동이 주어지면 다음상태를 확정적으로 알 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDrkgRQfuT4-",
        "colab_type": "text"
      },
      "source": [
        "**정책** : $U_t=\\pi(X_t)$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrnsGhWLuTy5",
        "colab_type": "text"
      },
      "source": [
        "정책은 상태변수에서 행동을 **직접 계산**한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3jxt9TfuTv_",
        "colab_type": "text"
      },
      "source": [
        "### **반환값($G_t$)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cs2OnG_2uTtF",
        "colab_type": "text"
      },
      "source": [
        "$\\rightarrow$ 시간스텝 $t$일 때, 미래에 얻을 수 있는 **보상의 총합**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76Ym3_U1uTpI",
        "colab_type": "text"
      },
      "source": [
        "$$G_t=\\sum_{k=t}^T  \\gamma^{k-t}r(X_k,U_k)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFsU-AW8uTmM",
        "colab_type": "text"
      },
      "source": [
        "$\\gamma \\in [0,1]$은 **감가율**이다. 값이 작을수록 보상을 가까운 미래에 받는다. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrEi5fdYuTjw",
        "colab_type": "text"
      },
      "source": [
        "### **에피소드**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t87CTO45uThU",
        "colab_type": "text"
      },
      "source": [
        ": 어떤 정책을 시행해 $X_0 \\rightarrow U_0 \\rightarrow r(X_0,U_0)\\rightarrow \\ldots\\rightarrow X_T\\rightarrow U_T$의 순서로 전개될 때, 이러한 **상태변수, 행동, 보상의 시퀀스 집합**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ua6EdqpnuTd5",
        "colab_type": "text"
      },
      "source": [
        "유한 구간 에피소드 : 특정 상태변수에 도달하는 등 목적이 성취되면 종료되는 에피소드($t=T$)\n",
        "<BR>무한 구간 에피소드 : $T=\\infty$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ek8Ok8WuTYz",
        "colab_type": "text"
      },
      "source": [
        "## 2.3.2 가치함수 - 상태가치 & 행동가치"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkaLffaouTVw",
        "colab_type": "text"
      },
      "source": [
        "##2.3.2.1 상태가치"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdHHpkS91BPZ",
        "colab_type": "text"
      },
      "source": [
        "![image](https://user-images.githubusercontent.com/53015968/81427186-66425800-9195-11ea-9d18-f9c1169d1fae.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TT2T9BJF0opw",
        "colab_type": "text"
      },
      "source": [
        "$\\rightarrow$ 상태변수($X_t$)에서 정책 $\\pi$에 의해 **행동이 가해졌을 때**, 기대할 수 있는 **반환값**.  즉, 미래 보상의 총합"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDPyV2AN1ZUx",
        "colab_type": "text"
      },
      "source": [
        "상태가치함수 식 : $V^\\pi(X_t)=E_{\\tau_{u_t:u_T \\sim p(\\tau_{u_t:u_T\\mid X_t})}}[\\sum_{k=t}^T \\gamma^{k-t}r(X_k,u_k)\\mid X_t]$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxfuzwIp1v6v",
        "colab_type": "text"
      },
      "source": [
        "#### 식의 인자의 의미"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su4kZiGP1bCw",
        "colab_type": "text"
      },
      "source": [
        "$p(\\tau_{u_t:u_T}\\mid X_t)$ $\\Rightarrow$  기댓값 계산 시, 확률밀도함수로 조건부 확률밀도함수 사용\n",
        "<br>$\\tau_{u_t:u_T} = (u_t,X_{t+1},u_{t+1},\\ldots,X_T,u_T)$ $\\Rightarrow$ 상태변수$X_t$에서 정책으로 생성되는 궤적"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5A09CoowuTTI",
        "colab_type": "text"
      },
      "source": [
        "## 2.3.2.2 행동가치"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNYRMFgb3Sde",
        "colab_type": "text"
      },
      "source": [
        "![image](https://user-images.githubusercontent.com/53015968/81427917-6d1d9a80-9196-11ea-97ea-f00797babb79.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucMYcvvd3T_R",
        "colab_type": "text"
      },
      "source": [
        "$\\rightarrow$ 상태변수($X_t$)에서  **행동($u_t$)을 선택하고**, 그로부터 정책($\\pi$)에 의해 행동이 가해졌을 때,기대할 수 있는 **반환값**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_YnhiHN3xwF",
        "colab_type": "text"
      },
      "source": [
        "행동가치 식 :  $Q^\\pi(X_t,u_t)=E_{\\tau_{x_{t+1}:u_T \\sim p(\\tau_{u_{t+1}:u_T\\mid X_t,u_t})}}[\\sum_{k=t}^T \\gamma^{k-t}r(X_k,u_k)\\mid X_t,u_t]$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nX6-7Fn30KL",
        "colab_type": "text"
      },
      "source": [
        "#### 식의 인자의 의미"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwhtO1lV30hu",
        "colab_type": "text"
      },
      "source": [
        "$p(\\tau_{u_{t+1}:u_T}\\mid X_t)$ $\\Rightarrow$  기댓값 계산 시, 확률밀도함수로 조건부 확률밀도함수 사용\n",
        "<br>$\\tau_{u_{t+1}:u_T} = (X_{t+1},u_{t+1},\\ldots,X_T,u_T)$ $\\Rightarrow$ 상태변수$X_t$에서 행동$u_t$를 선택하고 정책으로 생성되는 궤적"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtOEgeko30fI",
        "colab_type": "text"
      },
      "source": [
        "이 두 식을 이용하면 다음과 같은 결론을 얻을 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NnxxRh230cw",
        "colab_type": "text"
      },
      "source": [
        "$V^\\pi(X_t)=E_{u_t \\sim \\pi(u_t \\mid X_t)}[Q^\\pi (X_t,u_t)]$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ap4ayAWY30aP",
        "colab_type": "text"
      },
      "source": [
        "즉, **상태가치**는 상태변수$X_t$에서 **선택 가능한 모든 행동**$u_t$에 대한 **행동가치의 평균값**이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHqXnNtM30GD",
        "colab_type": "text"
      },
      "source": [
        "## 2.3.3 벨만 방정식"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFyaLHQL441I",
        "colab_type": "text"
      },
      "source": [
        ": **현재** 상태변수의 **가치**와 **다음** 시간스텝의 상태변수의 **가치**와의 관게"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeZhytoa45Qb",
        "colab_type": "text"
      },
      "source": [
        "$V^\\pi(X_t)=E_{u_t \\sim \\pi(u_t \\mid X_t)}[r(X_t,u_t)+E_{x_{t+1} \\sim p(X_{t+1} \\mid X_t,u_t)}[rV^\\pi(X_{t+1})]]$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCLX91nE45N4",
        "colab_type": "text"
      },
      "source": [
        "여기서 상태가치 함수($V^\\pi(X_t), V^\\pi (X_{t+1})$)는 원래 시변함수이므로 두 상태가치 함수는 다르다.\n",
        "<br>명확히 하기위해 위와 같이 명백히 표시해야한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alQOGakuAbhP",
        "colab_type": "text"
      },
      "source": [
        "하지만, 편의상  $T \\rightarrow \\infty$ (무한구간)이라면 두 상태 함수는 시불변함수가 되어 동일하게 된다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAIIErh045LY",
        "colab_type": "text"
      },
      "source": [
        "## 2.3.4 벨만 최적 방정식"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffMyIMQzBXDl",
        "colab_type": "text"
      },
      "source": [
        ": **현재의 최적 가치**와 **다음** 시간스텝의 **최적 가치**와의 관계"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FC_z6YyVCWhI",
        "colab_type": "text"
      },
      "source": [
        "**최적 정책**($\\pi^*(u_t \\mid X_t)$) : 상태가치의 값을 최대로 만드는 **정책**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQPSsUxJ45I_",
        "colab_type": "text"
      },
      "source": [
        "**최적 상태가치 함수** : 모든 정책 중 상태가치 값을 최대로 만드는 정책을 적용한 상태가치 함수\n",
        "<br>**최적 행동가치 함수** : 모든 정책 중 행동가치 값을 최대로 만드는 정책을 적용한 행동가치 함수"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxAXPC3BuTQQ",
        "colab_type": "text"
      },
      "source": [
        "최적 상태가치 함수는 최적 행동가치 함수의 최댓값을 취하는 아래와 같은 관계를 갖는다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pcEFNFQCC-B",
        "colab_type": "text"
      },
      "source": [
        "$V^*(X_t)=maxQ^*(X_t,u_t)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMsk5m_oCtNH",
        "colab_type": "text"
      },
      "source": [
        "##2.4 강화학습 방법"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luAOwyjBDhuS",
        "colab_type": "text"
      },
      "source": [
        "1. 가치함수 추정\n",
        "2. 직접 정책 유도\n",
        "3. 환경 모델 추정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8rFqzBZC68Y",
        "colab_type": "text"
      },
      "source": [
        "### 강화학습의 반복 과정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lp7_DK7zDseP",
        "colab_type": "text"
      },
      "source": [
        "위 세가지 방법은 아래와 같은 세가지의 공통점을 가진다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAL2uidkC_vN",
        "colab_type": "text"
      },
      "source": [
        "![2 9](https://user-images.githubusercontent.com/53015968/81430084-d05cfc00-9199-11ea-9deb-00c641339c66.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyDUe99VDA5r",
        "colab_type": "text"
      },
      "source": [
        "1. 정책 실행을 통한 데이터 생성\n",
        "2. 모델 또는 가치함수의 추정\n",
        "3. 정책 개선 \n",
        "<br>1 ~ 3단계 반복 $\\rightarrow$ 최적의 정책 산출"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVeFmkX5DHjx",
        "colab_type": "text"
      },
      "source": [
        "### 2.4.1 가치함수 추정 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SD_xCXcIEE6w",
        "colab_type": "text"
      },
      "source": [
        "$\\rightarrow$ **가치함수를 추정**하여 **최대의 보상**을 계산"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0B513XOEhKJ",
        "colab_type": "text"
      },
      "source": [
        "#### 가치 기반 강화학습 방법"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8H40ygIFBAC",
        "colab_type": "text"
      },
      "source": [
        "![2 10](https://user-images.githubusercontent.com/53015968/81430090-d226bf80-9199-11ea-8652-e6b5b5dce33c.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnulGV_IEMzJ",
        "colab_type": "text"
      },
      "source": [
        "1. 행동가치 함수를 추정\n",
        "2. 각 상태에서 행동가치 함수를 최대화하는 행동 선택\n",
        "3. 최적 정책 유도"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SSa7lBbD_gb",
        "colab_type": "text"
      },
      "source": [
        "대표적인 예 : DQN(deoop Q network) - 이산공간의 상태 & 행동 기반"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Z9_VEAzEx-D",
        "colab_type": "text"
      },
      "source": [
        "### 2.4.2 직접 정책 유도"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79qbxMTdFUX_",
        "colab_type": "text"
      },
      "source": [
        "목적 :  **보상의 기댓값을 최대**로 하기 위해 **정책($\\pi_\\theta (u_t \\mid X_t)$)**을 최적화하는 **정책 파라미터** $\\theta$를 계산"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Rp6f50BFxit",
        "colab_type": "text"
      },
      "source": [
        "#### 정책 그래디언트 방법"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnFKJC93E3OG",
        "colab_type": "text"
      },
      "source": [
        "![2 11](https://user-images.githubusercontent.com/53015968/81430093-d357ec80-9199-11ea-9e30-d9dc1fda779a.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzJtLcyBF1FM",
        "colab_type": "text"
      },
      "source": [
        "위 방법은 보통 가치함수도 함께 추정하여 정책의 성과를 평가하는 **Actor-Critic** 구조를 사용한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15R1F6RDG4jN",
        "colab_type": "text"
      },
      "source": [
        "+  Actor-Critic : 강화학습의 MDP 중 정책과 가치함수를 Deep Neural Network로 parameterization(파라미터를 이용해 전체를 예측)하여 근사화한 기법"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8sSGo5-FEan",
        "colab_type": "text"
      },
      "source": [
        "### 2.4.3 환경 모델 추정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4Y9kxdRHyys",
        "colab_type": "text"
      },
      "source": [
        "#### 모델 기반 강화학습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsG2TVEvIFlL",
        "colab_type": "text"
      },
      "source": [
        "![2 12](https://user-images.githubusercontent.com/53015968/81430098-d4891980-9199-11ea-83e2-6cd439c72da4.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxXCROjiH1ec",
        "colab_type": "text"
      },
      "source": [
        "모델 기반 강화학습은 간단하고 효율적이다. 또한 사용되는 정책 개선은 환경 모델을 정의하는 방식에 따라 다르다."
      ]
    }
  ]
}