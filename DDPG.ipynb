{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DDPG.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOC/QEgHsfoW6hd6HTuBFkS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/syeong1218/RL/blob/master/DDPG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehPbeur7RfK7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhywObLd04U_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "6459354f-4486-47cf-ea83-4fdcf29551cf"
      },
      "source": [
        "try:\n",
        "\n",
        "    %tensorflow_version 2.x  # %tensorflow_version only exists in Colab\n",
        "\n",
        "except Exception:\n",
        "\n",
        "    pass"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `2.x  # %tensorflow_version only exists in Colab`. This will be interpreted as: `2.x`.\n",
            "\n",
            "\n",
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fxb2Cm8XSCp6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Input, Lambda\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "class Actor(object):\n",
        "    \"\"\"\n",
        "        Actor Network for DDPG\n",
        "    \"\"\"\n",
        "    def __init__(self, state_dim, action_dim, action_bound, tau, learning_rate):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.action_bound = action_bound\n",
        "        self.tau = tau\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        self.model = self.build_network()\n",
        "        self.target_model = self.build_network()\n",
        "\n",
        "\n",
        "        self.actor_optimizer = tf.keras.optimizers.Adam(self.learning_rate)\n",
        "\n",
        "    ## actor network\n",
        "    def build_network(self):\n",
        "        state_input = Input((self.state_dim,))\n",
        "        h1 = Dense(64, activation='relu')(state_input)\n",
        "        h2 = Dense(32, activation='relu')(h1)\n",
        "        h3 = Dense(16, activation='relu')(h2)\n",
        "        out = Dense(self.action_dim, activation='tanh')(h3)\n",
        "\n",
        "        # Scale output to [-action_bound, action_bound]\n",
        "        action_output = Lambda(lambda x: x*self.action_bound)(out)\n",
        "        model = Model(state_input, action_output)\n",
        "        model.summary()\n",
        "        return model\n",
        "\n",
        "\n",
        "    ## actor prediction\n",
        "    def predict(self, state):\n",
        "\t\t# type of action in env is numpy array\n",
        "        return self.model.predict(np.reshape(state, [1, self.state_dim]))[0]\n",
        "\n",
        "\n",
        "    ## target actor prediction\n",
        "    def target_predict(self, state):\n",
        "        return self.target_model.predict(state)\n",
        "\n",
        "\n",
        "    ## transfer actor weights to target actor with a aau\n",
        "    def update_target_network(self):\n",
        "        theta, target_theta = self.model.get_weights(), self.target_model.get_weights()\n",
        "        for i in range(len(theta)):\n",
        "            target_theta[i] = self.tau * theta[i] + (1 - self.tau) * target_theta[i]\n",
        "        self.target_model.set_weights(target_theta)\n",
        "\n",
        "\n",
        "    ## train the actor network\n",
        "    def train(self, states, dq_das):\n",
        "        with tf.GradientTape() as tape:\n",
        "            self.dj_dtheta = tape.gradient(self.model(states), self.model.trainable_variables, -dq_das)\n",
        "        grads = zip(self.dj_dtheta, self.model.trainable_variables)\n",
        "        self.actor_optimizer.apply_gradients(grads)\n",
        "\n",
        "\n",
        "    ## save actor weights\n",
        "    def save_weights(self, path):\n",
        "        self.model.save_weights(path)\n",
        "\n",
        "\n",
        "    ## load actor wieghts\n",
        "    def load_weights(self, path):\n",
        "        self.model.load_weights(path + 'pendulum_actor.h5')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2TjkGKHSJKo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Input, concatenate\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "class Critic(object):\n",
        "    \"\"\"\n",
        "        Critic Network for DDPG: Q function approximator\n",
        "    \"\"\"\n",
        "    def __init__(self, state_dim, action_dim, tau, learning_rate):\n",
        "\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.tau = tau\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # create critic and target critic network\n",
        "        self.model = self.build_network()\n",
        "        self.target_model = self.build_network()\n",
        "\n",
        "        self.model.compile(optimizer=Adam(self.learning_rate), loss='mse')\n",
        "        self.target_model.compile(optimizer=Adam(self.learning_rate), loss='mse')\n",
        "\n",
        "\n",
        "    ## critic network\n",
        "    def build_network(self):\n",
        "        state_input = Input((self.state_dim,))\n",
        "        action_input = Input((self.action_dim,))\n",
        "        x1 = Dense(64, activation='relu')(state_input)\n",
        "        x2 = Dense(32, activation='linear')(x1)\n",
        "        #a1 = Dense(1, activation='linear')(action_input)\n",
        "        a1 = Dense(32, activation='linear')(action_input)\n",
        "        h2 = concatenate([x2, a1], axis=-1)\n",
        "        #h2 = Add()([x2, a1])\n",
        "        h3 = Dense(16, activation='relu')(h2)\n",
        "        q_output = Dense(1, activation='linear')(h3)\n",
        "        model = Model([state_input, action_input], q_output)\n",
        "        model.summary()\n",
        "        return model\n",
        "\n",
        "\n",
        "    ## q-value prediction of target critic\n",
        "    def target_predict(self, inp):\n",
        "        return self.target_model.predict(inp)\n",
        "\n",
        "\n",
        "    ## transfer critic weights to target critic with a aau\n",
        "    def update_target_network(self):\n",
        "        phi = self.model.get_weights()\n",
        "        target_phi = self.target_model.get_weights()\n",
        "        for i in range(len(phi)):\n",
        "            target_phi[i] = self.tau * phi[i] + (1 - self.tau) * target_phi[i]\n",
        "        self.target_model.set_weights(target_phi)\n",
        "\n",
        "\n",
        "    ## gradient of q-values wrt actions\n",
        "    def dq_da(self, states, actions):\n",
        "        a = tf.convert_to_tensor(actions)\n",
        "        with tf.GradientTape() as tape:\n",
        "            # compute dq_da to feed to the actor\n",
        "            tape.watch(a)\n",
        "            q = self.model([states, a])\n",
        "            q = tf.squeeze(q)\n",
        "        q_grads = tape.gradient(q, a)\n",
        "        return q_grads\n",
        "\n",
        "    ## single gradient update on a single batch data\n",
        "    def train_on_batch(self, states, actions, td_targets):\n",
        "        return self.model.train_on_batch([states, actions], td_targets)\n",
        "\n",
        "    ## save critic weights\n",
        "    def save_weights(self, path):\n",
        "        self.model.save_weights(path)\n",
        "\n",
        "\n",
        "    ## load critic wieghts\n",
        "    def load_weights(self, path):\n",
        "        self.model.load_weights(path + 'pendulum_critic.h5')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5liUY3GUSTAV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "class DDPGagent(object):\n",
        "\n",
        "    def __init__(self, env):\n",
        "\n",
        "        ## hyperparameters\n",
        "        self.GAMMA = 0.95\n",
        "        self.BATCH_SIZE = 64\n",
        "        self.BUFFER_SIZE = 20000\n",
        "        self.ACTOR_LEARNING_RATE = 0.0001\n",
        "        self.CRITIC_LEARNING_RATE = 0.001\n",
        "        self.TAU = 0.001\n",
        "\n",
        "        self.env = env\n",
        "        # get state dimension\n",
        "        self.state_dim = env.observation_space.shape[0]\n",
        "        # get action dimension\n",
        "        self.action_dim = env.action_space.shape[0]\n",
        "        # get action bound\n",
        "        self.action_bound = env.action_space.high[0]\n",
        "\n",
        "        ## create actor and critic networks\n",
        "        self.actor = Actor(self.state_dim,\n",
        "                           self.action_dim, self.action_bound, self.TAU, self.ACTOR_LEARNING_RATE)\n",
        "        self.critic = Critic(self.state_dim, self.action_dim, self.TAU, self.CRITIC_LEARNING_RATE)\n",
        "\n",
        "        ## initialize replay buffer\n",
        "        self.buffer = ReplayBuffer(self.BUFFER_SIZE)\n",
        "\n",
        "        # save the results\n",
        "        self.save_epi_reward = []\n",
        "\n",
        "    ## Ornstein Uhlenbeck Noise\n",
        "    def ou_noise(self, x, rho=0.15, mu=0, dt=1e-1, sigma=0.2, dim=1):\n",
        "        return x + rho*(mu - x)*dt + sigma*np.sqrt(dt)*np.random.normal(size=dim)\n",
        "\n",
        "    ## computing TD target: y_k = r_k + gamma*Q(s_k+1, a_k+1)\n",
        "    def td_target(self, rewards, q_values, dones):\n",
        "        y_k = np.asarray(q_values)\n",
        "        for i in range(q_values.shape[0]): # number of batch\n",
        "            if dones[i]:\n",
        "                y_k[i] = rewards[i]\n",
        "            else:\n",
        "                y_k[i] = rewards[i] + self.GAMMA * q_values[i]\n",
        "        return y_k\n",
        "\n",
        "\n",
        "    ## train the agent\n",
        "    def train(self, max_episode_num):\n",
        "\n",
        "        # initial transfer model weights to target model network\n",
        "        self.actor.update_target_network()\n",
        "        self.critic.update_target_network()\n",
        "\n",
        "        for ep in range(int(max_episode_num)):\n",
        "            # reset OU noise\n",
        "            pre_noise = np.zeros(self.action_dim)\n",
        "            # reset episode\n",
        "            time, episode_reward, done = 0, 0, False\n",
        "            # reset the environment and observe the first state\n",
        "            state = self.env.reset()\n",
        "            while not done:\n",
        "                # visualize the environment\n",
        "                #self.env.render()\n",
        "                # pick an action: shape = (1,)\n",
        "                action = self.actor.predict(state)\n",
        "                noise = self.ou_noise(pre_noise, dim=self.action_dim)\n",
        "                # clip continuous action to be within action_bound\n",
        "                action = np.clip(action + noise, -self.action_bound, self.action_bound)\n",
        "                # observe reward, new_state\n",
        "                next_state, reward, done, _ = self.env.step(action)\n",
        "                # add transition to replay buffer\n",
        "                train_reward = (reward + 8) / 8\n",
        "                self.buffer.add_buffer(state, action, train_reward, next_state, done)\n",
        "\n",
        "                if self.buffer.buffer_size > 1000:  # start train after buffer has some amounts\n",
        "\n",
        "                    # sample transitions from replay buffer\n",
        "                    states, actions, rewards, next_states, dones = self.buffer.sample_batch(self.BATCH_SIZE)\n",
        "                    # predict target Q-values\n",
        "                    target_qs = self.critic.target_predict([next_states, self.actor.target_predict(next_states)])\n",
        "                    # compute TD targets\n",
        "                    y_i = self.td_target(rewards, target_qs, dones)\n",
        "                    # train critic using sampled batch\n",
        "                    self.critic.train_on_batch(states, actions, y_i)\n",
        "                    # Q gradient wrt current policy\n",
        "                    s_actions = self.actor.model.predict(states) # shape=(batch, 1),\n",
        "                    # caution: NOT self.actor.predict !\n",
        "                    # self.actor.model.predict(state) -> shape=(1,1)\n",
        "                    # self.actor.predict(state) -> shape=(1,) -> type of gym action\n",
        "                    s_grads = self.critic.dq_da(states, s_actions)\n",
        "                    dq_das = np.array(s_grads).reshape((-1, self.action_dim))\n",
        "                    # train actor\n",
        "                    self.actor.train(states, dq_das)\n",
        "                    # update both target network\n",
        "                    self.actor.update_target_network()\n",
        "                    self.critic.update_target_network()\n",
        "\n",
        "                # update current state\n",
        "                pre_noise = noise\n",
        "                state = next_state\n",
        "                episode_reward += reward\n",
        "                time += 1\n",
        "\n",
        "            ## display rewards every episode\n",
        "            print('Episode: ', ep+1, 'Time: ', time, 'Reward: ', episode_reward)\n",
        "\n",
        "            self.save_epi_reward.append(episode_reward)\n",
        "\n",
        "\n",
        "            ## save weights every episode\n",
        "            #print('Now save')\n",
        "            self.actor.save_weights(\"./save_weights/pendulum_actor.h5\")\n",
        "            self.critic.save_weights(\"./save_weights/pendulum_critic.h5\")\n",
        "\n",
        "        np.savetxt('./save_weights/pendulum_epi_reward.txt', self.save_epi_reward)\n",
        "        print(self.save_epi_reward)\n",
        "\n",
        "\n",
        "    ## save them to file if done\n",
        "    def plot_result(self):\n",
        "        plt.plot(self.save_epi_reward)\n",
        "        plt.show()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epb78-YFSYV-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from collections import deque\n",
        "import random\n",
        "\n",
        "class ReplayBuffer(object):\n",
        "    \"\"\"\n",
        "    Reply Buffer\n",
        "    \"\"\"\n",
        "    def __init__(self, buffer_size):\n",
        "        self.buffer_size = buffer_size\n",
        "        self.buffer = deque()\n",
        "        self.count = 0\n",
        "\n",
        "    ## save to buffer\n",
        "    def add_buffer(self, state, action, reward, next_state, done):\n",
        "        transition = (state, action, reward, next_state, done)\n",
        "\n",
        "        # check if buffer is full\n",
        "        if self.count < self.buffer_size:\n",
        "            self.buffer.append(transition)\n",
        "            self.count += 1\n",
        "        else:\n",
        "            self.buffer.popleft()\n",
        "            self.buffer.append(transition)\n",
        "\n",
        "    ## sample a batch\n",
        "    def sample_batch(self, batch_size):\n",
        "        if self.count < batch_size:\n",
        "            batch = random.sample(self.buffer, self.count)\n",
        "        else:\n",
        "            batch = random.sample(self.buffer, batch_size)\n",
        "        # return a batch of transitions\n",
        "        states = np.asarray([i[0] for i in batch])\n",
        "        actions = np.asarray([i[1] for i in batch])\n",
        "        rewards = np.asarray([i[2] for i in batch])\n",
        "        next_states = np.asarray([i[3] for i in batch])\n",
        "        dones = np.asarray([i[4] for i in batch])\n",
        "        return states, actions, rewards, next_states, dones\n",
        "\n",
        "\n",
        "    ## Current buffer occupation\n",
        "    def buffer_size(self):\n",
        "        return self.count\n",
        "\n",
        "    ## Clear buffer\n",
        "    def clear_buffer(self):\n",
        "        self.buffer = deque()\n",
        "        self.count = 0"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQPT13HuSqui",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "aea62d71-1a0f-472e-e08c-13471a31e6ec"
      },
      "source": [
        "import gym\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    max_episode_num = 200\n",
        "    env = gym.make(\"Pendulum-v0\")\n",
        "    agent = DDPGagent(env)\n",
        "\n",
        "    agent.train(max_episode_num)\n",
        "\n",
        "    agent.plot_result()\n",
        "\n",
        "\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 3)]               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 17        \n",
            "_________________________________________________________________\n",
            "lambda (Lambda)              (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 2,881\n",
            "Trainable params: 2,881\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"functional_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 3)]               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1)                 17        \n",
            "_________________________________________________________________\n",
            "lambda_1 (Lambda)            (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 2,881\n",
            "Trainable params: 2,881\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"functional_5\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 64)           256         input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_4 (InputLayer)            [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 32)           2080        dense_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_10 (Dense)                (None, 32)           64          input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 64)           0           dense_9[0][0]                    \n",
            "                                                                 dense_10[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_11 (Dense)                (None, 16)           1040        concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_12 (Dense)                (None, 1)            17          dense_11[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 3,457\n",
            "Trainable params: 3,457\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"functional_7\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            [(None, 3)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_13 (Dense)                (None, 64)           256         input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_6 (InputLayer)            [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_14 (Dense)                (None, 32)           2080        dense_13[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_15 (Dense)                (None, 32)           64          input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 64)           0           dense_14[0][0]                   \n",
            "                                                                 dense_15[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_16 (Dense)                (None, 16)           1040        concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_17 (Dense)                (None, 1)            17          dense_16[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 3,457\n",
            "Trainable params: 3,457\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Episode:  1 Time:  200 Reward:  -1196.892415790861\n",
            "Episode:  2 Time:  200 Reward:  -1150.2111866122623\n",
            "Episode:  3 Time:  200 Reward:  -1567.9928044140713\n",
            "Episode:  4 Time:  200 Reward:  -1394.13849196519\n",
            "Episode:  5 Time:  200 Reward:  -1318.731148126714\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cqqsgj2WSx4B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}