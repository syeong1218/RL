{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL_Ch2-1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMGheFaNSDVEWKJ8DK1fxQd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/syeong1218/RL/blob/master/RL_Ch2_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISGqpSpuCMU2",
        "colab_type": "text"
      },
      "source": [
        "# 2장 강화학습 개념"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsvD2wn8CRj3",
        "colab_type": "text"
      },
      "source": [
        "### 2.1 강화학습 개요\n",
        "- 강화학습 : 원하는 목표를 달성하기 위해 시간 순서대로 시스템에 가해지는 행동(action)을 선택하기 위한 방법.\n",
        "\n",
        "  - 구성 요소 : 에이전트, 환경, 에이전트와 환경과의 상호작용 행동, 보상, 상태의 관측\n",
        "  - 시간 변수는 불연속적이며 모든 시간스텝마다 행동이 가해진다.\n",
        "  - 의사 결정자 : 에이전트(agent)\n",
        "  - 시스템 : 에이전트의 환경(environment)\n",
        "  - agent는 환경의 변화를 표현하는 상태(state)을 관측(dbservation)하여 일정한 정책(policy)하에 불연속적인 값이나 연속적인 값으로 된 행동을 선택하여, 이를 환경에 인가해 환경을 변화시기킨다. 그 결과 시간스텝마다 의사 결정 성과를 평가하는 보상(reward)를 제공받는다.\n",
        "  - 강화학습의 목표 : **종료되는 시점까지 누적된 총 보상을 최대화하는 것.**\n",
        "  - 순차적인 의사 결정의 특징\n",
        "\n",
        "    : 일정 시간스텝에서 일시적으로 보상이 작더라도 누적 보상을 최대로 할 수 있는 행동을 선택.\n",
        "   \n",
        "- 예시 1 : 그리드 월드\n",
        "\n",
        "![대체 텍스트](https://github.com/syeong1218/RL/blob/master/2-1.jpg?raw=true)\n",
        "\n",
        "목표 : 로봇이 (4,3)으로 스스로 찾아가게 해서 최대 보상을 받는 것\n",
        "\n",
        "에이전트 : 이동방향을 결정하는 로본의 컴퓨터\n",
        "\n",
        "환경 : 에이전트를 제외한 로봇과 그리드 월드 전체\n",
        "\n",
        "상태 : 그리드 월드 내에서 로봇의 위치\n",
        "\n",
        "행동 : 상(U), 하(D), 좌(L), 우(R)\n",
        "\n",
        "=> 로봇의 위치를 바꿔가며 받는 보상을 통해 시행착오를 겪으면서 R-R-U-U-R,  R-R-R-L-U-U-L-L-R-R-R과 같은 행동을 선택한다. 로봇이 이동할 때마다 보상 -0.01을 받는다고 하면  R-R-U-U-R이나 U-U-R-R-R 중 하나를 선택 할 것이다.\n",
        "\n",
        "- 최적 제어와 강호학습의 차이점\n",
        "\n",
        " : **제어 대상 시스템의 수학적 동역학 모델의 유무**\n",
        "    - 최적제어 : 수학적 모델을 가지고있어 시스템이 어떻게 동작할지 예측 가능하다.\n",
        "    - 강화학습 : 수학적 모델을 요구하지 않고 시스템에서 얻은 데이터만 사용한다.\n",
        "\n",
        "    => 다양한 변수들로 인해 시스템의 모델을 전혀 얻을 수 없는 경우가 있는데, 이 경우에는 강화학습이 좋다.\n",
        "\n",
        "    - 예시 : 드론이 장애물을 피해가면서 최소의 에너지를 사용해 목표 지점에 도달하도록 드론에 가해지는 제어 입력 또는 행동을 순차적으로 결정하는 것이 목표.\n",
        "\n",
        "![대체 텍스트](https://github.com/syeong1218/RL/blob/master/2-3.jpg?raw=true)\n",
        "\n",
        "| |최적제어|강화학습|\n",
        "|----|----|----|\n",
        "|비행 제어 컴퓨터|제어기|에이전트|\n",
        "|컴퓨터를 제외한 모든 것|시스템|환경|\n",
        "||수학적 모델에 의존|보상에 의존|\n",
        "\n",
        "![대체 텍스트](https://github.com/syeong1218/RL/blob/master/2-4.jpg?raw=true)\n",
        "=> 최적 제어는 비행 제어 컴퓨터를 제외한 모든 환경에 대한 수학적 모델을 알고 있고, 강화학습은 아무것도 모른다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kFa6UBGCUg7",
        "colab_type": "text"
      },
      "source": [
        "### 2.2 강화학습 프로세스와 표기법\n",
        "\n",
        "![대체 텍스트](https://github.com/syeong1218/RL/blob/master/2-5.jpg?raw=true)\n",
        "\n",
        "(1) 에이전트는 환경의 상태($x_t$)를 측정한다.\n",
        "\n",
        "(2) 측정한 상태에서 에이전트의 정책으로 선택한 행동($u_t$)을 환경에 보낸다.\n",
        "\n",
        "    정책 : 측정한 상태를 바탕으로 최선의 행동으 ㄹ선택하기 위한 에이전트의 규칙 또는 방법\n",
        "\n",
        "(3) 행동에 의해서 환경의 상태는 다음 상태($x_{t+1}$)로 전환된다.\n",
        "\n",
        "(4) 전환된 환경의 상태를 바탕으로 다시 에이전트는 새로운 행동을 선택한다.\n",
        "\n",
        " (5) 환경으로부터 주어지는 즉각적인 보상($r_{t+1}$)을 사용해 장기적인 성과를 계산 또는 예측해서 에이전트의 정책을 개선한다.\n",
        "\n",
        " => 이와 같이 시간스텝마다 에이전트가 새로운 보상을 받는 것을 반복하는 과정을 수학적으로 모델링한 것이 마르코프 결정 프로세스(MDP)이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLHDPoBwFrrJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}